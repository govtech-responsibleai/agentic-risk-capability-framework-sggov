# :material-shield-alert: Why the ARC Framework?

In this section, we explain why we developed the ARC framework, how it fits into the existing literature, and why we chose to focus on capabilities specifically.

## Background

OpenAI dubbed 2025 the "year of the AI agent”,[^1] a prediction which quickly proved prescient. Within the first half of the year, major companies launched increasingly powerful systems that allow large language model (LLM) agents to reason, plan, and autonomously execute tasks such as code development or trip planning.[^2] However, this surge in agent-driven AI innovation also brought renewed scrutiny to these systems' safety and security risks. Recent research has shown that LLM agents are often more prone to unsafe behaviors than their base models,[^3] partly due to their growing autonomy and expanded capabilities through tool integration. As the ecosystem for agentic systems, including frameworks like Model Context Protocol[^4], continues to mature, there is greater urgency to establish robust governance mechanisms. 

## Existing Work on Technical Frameworks

<!-- ### Regulatory frameworks

<font color="red">While regulatory frameworks such as the EU AI Act[^5] and NIST Risk Management Framework[^6] provide overarching principles and guidelines to manage AI risks, they are too high-level and conceptual for organizations to meaningful translate into governance policies. [to be edited]</font>

### Technical frameworks -->

As increased autonomy represents reduced determinism and predictability of agentic systems, many technical frameworks have focused on assessing the risks from a security perspective, adapting from cybersecurity literature. This includes Cloud Security Alliance's MAESTRO framework[^5] and OWASP's whitepaper on agentic AI risks[^6], which focus on **threat modeling approaches** to identify risks and mitigations. These approaches detail security threats agentic systems could face (e.g., data poisoning, agent impersonation, cascading hallucination) and provide accompanying mitigations. Developers are expected to map out the system's architecture, and then identify attack vectors in each component from the list of risks. However, this is an onerous and challenging process for non-cybersecurity developers, as they are expected to enumerate across all possible risks for each component. NVIDIA's recommended taint tracing approach[^7] similarly requires developers to identify security boundaries in the system's architecture with respect to the flow of untrusted data, assess the sensitivity of tools impacted by the flow and then adopt appropriate security controls accordingly. However, unlike OWASP and MAESTRO, the recommended controls are not as comprehensive and largely entail human oversight, which has severe limitations.

Academic literature has focused on producing **safety and security benchmarks** to assess agentic AI risks. These benchmarks entail test scenarios or tasks that are expected to reveal specific risk behaviours of the agentic system. For example, CVEBench,[^8] RedCode,[^9] AgentHarm,[^10], AgentDojo[^11] assess whether LLMs can complete cybersecurity attacks or harmful tasks like fraud. In these settings, agentic systems are assessed on whether they comply with and successfully complete multi-step requests. However, these benchmarks do not help developers systematically identify the risks and attack scenarios of their specific applications. **Tool-based benchmarks** that entail evaluating the risks of individual tools that agents have access to face similar problems. APIBench,[^12] ToolSword,[^13], and ToolEmu[^14], evaluate the performance and safety of LLMs in utilizing and interacting with tools like `bash`. However, there exists agentic system risks unrelated to tool use (e.g., using a misaligned planning agent resulting in bad plans) which such benchmarks will not cover. As such, they are insufficient in helping developers assess agentic risks. 

In terms of technical controls for managing risks, **AI control** has emerged as a useful paradigm in preventing misaligned AI systems from causing harm.[^15] Rather than relying solely on training techniques to shape model behavior, AI control focuses on designing mechanisms like monitoring and human approval to constrain AI systems. In line with this paradigm, some works have emerged to generate custom controls for specific applications. Progent introduces a language for flexibly expressing privilege control policies applied during execution for each agent.[^16] The UK AI Security Institute similarly advocates for application-specific controls, which is derived from evaluating each system's threat model-specific capabilities based on models and deployment contexts.[^17]

<!-- To measure agentic AI risks, task-based frameworks have emerged to evaluate the ability of LLMs to execute malicious tasks. Benchmarks like CVEBench,[^7] CyBench,[^8] AgentHarm,[^9] and AgentDojo[^10] help to assess whether LLMs can complete cybersecurity attacks or harmful tasks like fraud. In these settings, agentic systems are required to not only comply with but also complete multi-step malicious requests. However, the ever-evolving landscape of attack scenarios make it difficult for organizations to constantly maintain a collection of harmful tasks and test their systems against them. This is also impractical when organizations deploy several agentic AI systems with differing goals. -->

<!-- Other works focus on evaluating the risks of individual tools that agents have access to. Tool-based benchmarks, like APIBench,[^11] ToolSword,[^12], and ToolEmu[^13], evaluate the performance and safety of LLMs in utilizing and interacting with tools like `bash`. However, as more tools are developed, tool-based approaches are too granular and difficult to maintain and operationalize at the organization level.  -->

Instead of providing a list of risks and controls upfront, some works have instead published guides on **best practices** in evaluating and managing risks of agentic AI systems. OpenAI[^18] shared best practices like constraining the agent's action spaces, setting default behaviors and ensuring attributabilty, as well as open problems for further investigation. Google[^19] emphasises a hybrid defence-in-depth strategy that combines traditional, deterministic security measures with dynamic, reasoning-based defenses, and higlights core principles of human controls, agent permissions and observability in designing agentic systems. Similarly, Beurer-Kellner et al.[^20] propose six design patterns for building AI agents with provable resistance to prompt injection, based on the principle that once an LLM agent has ingested untrusted input, it must be constrained so that it is impossible for that input to trigger any consequential actions.

## What is different about the ARC? 

The ARC builds on existing works to enable organisations to govern agentic systems effectively and provide practical guidance in assessing and mitigating the associated risks. Importantly, it attempts to structure the risks and controls identified in existing works into a unified and actionable framework. This is achieved mainly by including and differentiating between **baseline** and **capability** risks and controls. With a structured approach, the ARC attempts to strike an appropriate balance between:

* **Thoroughly mapping out the risk landscape, and not being too inflexible to changes** - Instead of listing out all possible agentic risks upfront, the ARC ties capabilities to risks, providing flexibility for identifying more risks as agentic systems acquire more capabilities
* **Being applicable and adaptable to all agentic AI systems, and not being too generic or watered down** - The ARC combines both generic (i.e., baseline) and specific (i.e., capability) risks and controls to ensure broad applicability and minimal standards while mantaining customisability
* **Providing meaningful and concrete guidance on the risks and controls, and not being too prescriptive** - Contextualising risk identification from a capabilities perspective is simpler and more straightforward for developers, resulting in practical and proportional risk assessment and mitigation
* **Scalability at an organisation-level, and not simply being a paper exercise** - The ARC provides an initial set of risks and controls to reduce compliance effort and ensure scalability across large organisations, while the customisation process ensures proper risk assessment

<!--   
However, identifying risk at the individual agent level makes it difficult for organizations to prescriptively and preemptively manage risks across many different teams and systems. Hence, we build on these frameworks and introduce a structured taxonomy of ***capabilities*** that agents can acquire. This enables organizations to operationalize agentic risk management by establishing and maintaining a taxonomy of risk categories and controls at the capabilities level, while giving teams the flexibility in how to implement the controls.  -->

<!-- <font color="red">## Why focus on capabilities?

Beyond the common components and design patterns, there is an overwhelming diversity of use cases and applications for agentic AI systems across various industries and horizontals. For example, agents have been adopted by companies to [accelerate paperwork and document writing for scientific research](https://www.anthropic.com/customers/bluenote), [support merchants on ecommerce platforms](https://www.grab.com/sg/press/others/grab-deploys-agentic-ai-to-empower-merchants-and-driver-partners/), and [improve customer service across various industries](https://cloud.google.com/transform/101-real-world-generative-ai-use-cases-from-industry-leaders). 

As such, **a monolithic standard for agentic risks and controls is simply not effective** - governance frameworks need flexibility to tailor risk identification and mitigation to the specifics of their organisation's context and the system's use case. However, good frameworks also need to provide meaningful guidance and establish a minimal baseline across the organisation. Leaving it completely up to system owners is not ideal either.

While misaligned models present a fundamental challenge, misalignment alone is not inherently dangerous if the model lacks the ability to act. A misaligned but powerless model is unable to cause much harm. For instance, even if an adversary tricks an agent into generating malicious code, the risk remains low if the agent cannot execute that code in a live environment. This underscores a central insight of our work: **risk arises not just from what an agent can think or say, but more from what it can do**. Much of the current literature has focused on vulnerabilities introduced by granting agents access to specific tools, such as web browsers, email APIs, or code execution environments. However, these analyses are often tool-specific and do not generalize well across the fast-evolving agentic ecosystem.

because there are multiple tools that could enable the same capability (for example, a local `bash` terminal, versus one in a Docker container, versus one in an EC2 instance), and a single tool can enable multiple capabilities (for example, GitHub's MCP server enables the LLM to list all existing repositories, examine the code in each one of them, push code changes via a Git commit, and even merge a pull request). For a deeper discussion,


We advocate instead for a capability-centric view of agentic AI governance. We define **capabilities as the general classes of actions an agentic system can perform, given the tools, memory, and instructions available to it**. Crucially, capabilities are distinct from tools: a single tool may enable multiple capabilities, and conversely, a single capability may be enabled by multiple tools. For example, access to a user's Facebook account (a single tool) may enable capabilities of communication (e.g., messaging contacts) and transactions (e.g., updating account details) respectively. In contrast, the capability of web search may be implemented via a variety of tools.

We argue that effective governance should focus on capabilities rather than tools for the following reasons:

1. Impact-focused precision: It is the action performed, not the specific tool used, that poses risk. A capability-centric model allows governance to focus on the consequences of actions, ensuring more consistent treatment across different implementations.
2. Action-level granularity: A single tool can enable different types of actions, and these will need to be governed differently from each other. Governing at the capability level allows organizations to tailor controls more precisely to each type of action.
3. Scalability across platforms: The agentic tool ecosystem is vast and rapidly growing. A governance approach tied to specific tools does not generalize well. A capability-centric framework abstracts away tool dependencies and provides a scalable foundation for managing diverse systems. 

[to be edited]</font> -->

 <!--- Footnotes below --->

[^1]: Hamilton, E. 2025 is the year of ai agents, OpenAI CPO says. Axios, January 2025. URL <https://www.axios.com/2025/01/23/davos-2025-ai-agents>. Accessed: 2025-05-11.
[^2]: Both Anthropic and Google released agentic tools ([Claude Code](https://www.anthropic.com/solutions/agents) and [Gemini CLI](https://blog.google/technology/developers/introducing-gemini-cli-open-source-ai-agent/) respectively), built on top of their popular models. Most recently, OpenAI released ChatGPT Agent in July 2025 ([article](https://openai.com/index/introducing-chatgpt-agent/)).
[^3]: See the following articles: (i) Chiang et al. Harmful helper: Perform malicious tasks? web AI agents might help. In ICLR 2025 Workshop on Building Trust in Language Models and Applications, 2025. URL <https://openreview.net/forum?id=4KoMbO2RJ9>, (ii) Kumar et al. Aligned LLMs are not aligned browser agents. In The Thirteenth International Conference on Learning Representations, 2025. URL <https://openreview.net/forum?id=NsFZZU9gvk>, (iii) Yu, C. and Papakyriakopoulos, O. Safety devolution in AI agents. In ICLR 2025 Workshop on Human-AI Coevolution, 2025. URL <https://openreview.net/forum?id=7nJmuFFkWd>.
[^4]: Anthropic. Model Context Protocol (MCP). <https://docs.anthropic.com/en/docs/agents-and-tools/mcp>, 2024. Accessed: 2025-05-11.
[^5]: Ken Huang. Agentic AI Threat Modeling Framework: MAESTRO. <https://cloudsecurityalliance.org/blog/2025/02/06/agentic-ai-threat-modeling-framework-maestro>, 2025. Accessed: 2025‑07‑26.
[^6]: OWASP Foundation. Agentic AI – Threats and Mitigations. <https://genai.owasp.org/resource/agentic-ai-threats-and-mitigations/>, 2025. Accessed: 2025‑07‑26.
[^7]: Harang et al. Agentic Autonomy Levels and Security. <https://developer.nvidia.com/blog/agentic-autonomy-levels-and-security/>, 2025. Accessed: 2025‑07‑26.
<!-- [^5]: European Parliament and Council of the European Union. Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 Laying down harmonised rules on artificial intelligence (Artificial Intelligence Act). <https://eur-lex.europa.eu/eli/reg/2024/1689/oj/eng>, 2024. Accessed: 2025- 05-11.
[^6]: National Institute of Standards and Technology. NIST AI Risk Management Framework Playbook. <https://www.nist.gov/itl/ai-risk-management-framework/nist-ai-rmf-playbook>, 2023. Accessed: 2025-05-11. -->
[^8]: Zhu et al. CVE-Bench: A Benchmark for AI Agents' Ability to Exploit Real-World Web Application Vulnerabilities. <https://arxiv.org/abs/2503.17332>, 2024. Accessed: 2025-05-11.
[^9]: Guo et al. RedCode: Risky Code Execution and Generation Benchmark for Code Agents. NeurIPS 2024 Datasets and Benchmarks Track. <https://proceedings.neurips.cc/paper_files/paper/2024/hash/bfd082c452dffb450d5a5202b0419205‑Abstract‑Datasets_and_Benchmarks_Track.html>, 2024. Accessed: 2025‑07‑26.
[^8]: Zhang et al. Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risks of Language Models. <https://arxiv.org/abs/2408.08926>, 2024. Accessed: 2025-05-11.
[^10]: Andriushchenko et al. AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents. <https://arxiv.org/abs/2410.09024>, 2025. Accessed: 2025-05-11.
[^11]: Debenedetti et al. AgentDojo: A Dynamic Environment to Evaluate Prompt Injection Attacks and Defenses for LLM Agents. <https://arxiv.org/abs/2406.13352>, 2024. Accessed: 2025-05-11.
[^12]: Patil et al. Gorilla: Large Language Model Connected with Massive APIs. <https://arxiv.org/abs/2305.15334>, 2023. Accessed: 2025-05-11.
[^13]: Ye et al. ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages. <https://arxiv.org/abs/2402.10753>, 2024. Accessed: 2025-05-11.
[^14]: Ruan et al. Identifying the Risks of LM Agents with an LM-Emulated Sandbox. <https://arxiv.org/abs/2309.15817>, 2023. Accessed: 2025-05-11.
[^15]: Greenblatt et al. arXiv preprint arXiv:2312.06942, 2023. https://arxiv.org/abs/2312.06942, accessed: 2025‑07‑26. 
[^16]: Shi et al. Progent: Programmable Privilege Control for LLM Agents. <https://arxiv.org/abs/2504.11703>, 2025. Accessed: 2025-05-11.
[^17]: Korbak et al. How to evaluate control measures for LLM agents? A trajectory from today to superintelligence. <https://arxiv.org/abs/2504.05259>, 2025. Accessed: 2025-05-11.
[^18]: Shavit et al. Practices for Governing Agentic AI Systems. OpenAI white paper, 2023. <https://cdn.openai.com/papers/practices-for-governing-agentic-ai-systems.pdf>, Accessed: 2025‑07‑26.
[^19]: Díaz et al. An Introduction to Google’s Approach for Secure AI Agents. Google white paper, 2025. <https://research.google/pubs/an-introduction-to-googles-approach-for-secure-ai-agents/>, Accessed: 2025‑07‑26.
[^20]: Beurer‑Kellner et al. Design Patterns for Securing LLM Agents against Prompt Injections. arXiv preprint arXiv:2506.08837, first posted June 10, 2025; revised versions through June 27, 2025. <https://arxiv.org/abs/2506.08837>, Accessed: 2025‑07‑26.