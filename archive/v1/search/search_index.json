{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Agentic Risk &amp; Capability Framework","text":"<p>We introduce the Agentic Risk &amp; Capability (\"ARC\") Framework, a technical governance framework that enables organisations to manage the safety and security risks of agentic AI systems through a risk-based approach. We do this by:</p> <ul> <li>Defining a hierarchical taxonomy of capabilities that agentic AI systems may have, depending on their use case and how they are designed  </li> <li>Distinguishing between baseline risks (applicable to all agentic AI systems) and capability-specific risks (applicable to agentic AI systems with that capability)  </li> <li>Mapping each risk to a set of technical controls which help to mitigate the risk to an acceptable level  </li> <li>Providing a framework to scale governance of agentic AI systems, especially for large organisations   </li> </ul>"},{"location":"#this-website","title":"This website","text":"<p>We have organised our content into four sections:</p> <ol> <li>Introduction: We explain the overall concept of the ARC Framework, how it distinguishes between baseline risks and capability-specific risks, and how it fills a much-needed gap in current discussions on agentic AI governance.   </li> <li>Baseline: We explain what baseline risks are, outline a set of baseline risks that apply to all agentic AI systems, and list the corresponding technical controls for tackling those risks.  </li> <li>Capabilities: We explore the concept of capabilities, go through our proposed taxonomy of agentic capabilities, describe the safety and security risks arising from each capability, and set out the relevant technical controls we recommend for mitigating those risks.  </li> <li>Implementation: We provide a plan for operationalising the ARC framework in an organisation, using stylised examples to help the reader understand the practical implications of the framework.  </li> </ol> <p>For first time readers, we suggest following the order of the contents above. For those who are more familiar with agentic AI governance or with the ARC framework, please feel free to jump ahead to the relevant sections.</p>"},{"location":"#about-us","title":"About us","text":"<p>The ARC Framework is developed by the Responsible AI team in GovTech Singapore's AI Practice. We develop deep technical capabilities in Responsible AI to improve how the Singapore government develops, evaluates, deploys, and monitors AI systems in a safe, trustworthy, and ethical manner.</p> <p>In developing this framework, we work closely with other teams in the Singapore government, such as the Ministry for Digital Development and Information and the Cybersecurity Agency of Singapore. We are grateful for their feedback and contributions, which have helped to make this framework more effective, robust, and thorough.</p> <p>To reach out to us, please fill out the Google form here.</p>"},{"location":"#citing-our-work","title":"Citing our work","text":"<p>To cite our work, please use the following BibText citation:</p> <pre><code>@article{agentic_risk_capability_framework,\n    title   = {Agentic Risk &amp; Capability Framework},\n    author  = {Khoo, Shaun and Foo, Jessica and Lee, Roy Ka-Wei},\n    year    = {2025},\n    month   = {July},\n    url     = {https://govtech-responsibleai.github.io/agentic-risk-capability-framework/}\n}\n</code></pre> <p>Alternatively, you may use the APA-formatted citation below:</p> <p>Khoo, S. &amp; Foo, J. &amp; Lee, R. K.-W. (2025) Agentic Risk &amp; Capability Framework. URL https://govtech-responsibleai.github.io/agentic-risk-capability-framework/</p> <p> Note: This page was last updated on 7 Aug 2025.</p>"},{"location":"baseline/","title":"What is a Baseline?","text":"<p>There are common aspects across all agentic AI systems - the agents themselves have similar components (LLM, tools, instructions, memory), while the system itself has some architectural requirements for the \"agentic\" part to work effectively. In this section, we describe each of these aspects and explain how they may give rise to certain risks (but we defer the full treatment of the risks to the next section).</p> <p>Click here for a downloadable version.</p>"},{"location":"baseline/#components-of-an-agent","title":"Components of an Agent","text":"<p> Figure 1: Components of a typical agent in an agentic AI system</p> <p>Based on our literature review of agents, we observe that individual agents are typically composed of four key components as shown in Figure 1 above. We go through them in turn below.</p>"},{"location":"baseline/#llm","title":"LLM","text":"<p>The LLM is the central reasoning engine that processes instructions, interprets user inputs, and generates contextually appropriate responses by leveraging its trained language understanding and generation capabilities. As the cognitive core of the agent, it orchestrates all other components, such as determining when to access memory, which tools to invoke, and how to synthesize information into coherent outputs that align with the given instructions.</p> <p>Teams today have a wide range of LLMs to pick from - from large closed-source LLMs such as OpenAI's GPT o3, Anthropic's Claude 4 Opus, or Gemini 2.5 Pro, to smaller open-weights models like Llama 3.3 8B, Qwen3 8B, and Mistral Small 3.1. Moreover, some teams may choose to finetune models to achieve greater performance for their specific use case or domain.</p> <p>The choice of LLM has significant implications for the safety and security of the agent - some LLMs are more prone to being jailbroken than others, while others are less aligned and thus more likely to generate malicious or harmful outputs. In addition, using LLMs which are hosted by external model providers versus hosting it internally on your own infrastructure carries different risks.</p>"},{"location":"baseline/#tools","title":"Tools","text":"<p>What sets agents apart from standard LLM applications is the ability to autonomously execute actions, and the most critical enabler of that is tool use. Tools transform the agent from a passive conversational system into an active problem-solver that can manipulate files, query databases, control devices, or access APIs based on the LLM's reasoning and user needs. Underlying the ecosystem of tools for agents is the Model Context Protocol (\"MCP\"), which provides a consistent interface for LLMs to discover and interact with a variety of external tools and services, thereby enabling simpler actions from executing code on the local <code>bash</code> terminal as well as more complex ones like merging pull requests on GitHub automatically. </p> <p>Tools are closely related to capabilities because tools enable capabilities - executing code or reading databases would not be possible without a MCP server to bridge between the LLM and the external tool (i.e. the <code>bash</code> terminal or the Postgres database). However, tools are distinct from capabilities (see the previous section Why the ARC Framework? for the full discussion), and we analyse them as a distinct component of an agent.</p> <p>While the risks relating to tools tend to be linked to their capabilities (e.g. code execution risks arise from having access to the <code>bash</code> terminal), there are some common risks which are shared by all tools, such as weak authentication protocols, rogue MCP tools, or vulnerability to prompt injections. Basic hygiene standards for the use of tools in agentic AI systems may be needed to manage these risks.</p>"},{"location":"baseline/#instructions","title":"Instructions","text":"<p>Instructions are the blueprint which defines an agent's role, capabilities, and behavioural constraints, ensuring it operates within intended parameters and maintains its performance across different scenarios. Without clear instructions, even sophisticated LLMs with powerful tools and extensive knowledge bases would  produce inconsistent or misaligned outputs that fail to meet user expectations or safety requirements.</p> <p>Due to the importance of instructions in guiding the agent's behaviour, poor instructions can give rise to serious safety and security risks in an agentic AI system. For example, incorrectly defined objectives can result in agents optimising for the wrong metric, which either renders it ineffective or leads to unforeseen safety and security consequences for the organisation. Furthermore, a lack of instructions on the proper ethical or safety guidelines may cause the LLM to take actions which are contradictory to the organisation's or user's well-being.</p>"},{"location":"baseline/#memory","title":"Memory","text":"<p>The memory or knowledge base component provides the agent with contextual awareness and information persistence, enabling it to maintain coherent conversations, learn from past interactions, and access relevant facts without requiring constant re-instruction. For more complex multi-agent systems, the memory component is critical for the agent to operate collaboratively with other agents in the system and to retain the contextual understanding necessary for complex, multi-step tasks.</p> <p>Memory components are vulnerable to data poisoning where malicious actors inject false or misleading information that gets stored and later influences the agent's decisions. Privacy and data leakage risks may also occur if agents inadvertently stores and shares sensitive information from previous interactions. There may also be persistence risks where temporary errors, biases, or inappropriate responses become permanently encoded in the knowledge base, reinforcing problematic behaviors over time and making the agent increasingly unreliable.</p>"},{"location":"baseline/#design-of-an-agentic-system","title":"Design of an agentic system","text":"<p>We now broaden our perspective to examine how agentic AI systems are assembled from individual agents, focusing specifically on two aspects - the agentic architecture (i.e. how agents are connected) and their roles and access controls (i.e. how the task is split across agents).</p>"},{"location":"baseline/#agentic-architecture","title":"Agentic Architecture","text":"<p>The agentic architecture defines how multiple agents are interconnected, coordinated, and orchestrated to collectively solve complex tasks that exceed individual agent capabilities, including patterns like hierarchical delegation, parallel processing, or sequential handoffs between specialised agents. One key aspect of any agentic architecture is the level of agentic autonomy, or the range of decisions that agents are permitted to make.<sup>1</sup> </p> <p> Figure 2: Illustration of two different agentic architectures</p> <p>Some agentic AI systems may use a linear workflow for greater control and predictability, while others may use a centralised orchestration approach with specialised agentic roles to speed up the processing of many data sources. For example, a customer support system may use a linear agentic workflow where queries pass sequentially through classification, research, solution generation, and quality assurance agents, while an internal analytics system may spin up multiple agents simultaneously to access different data sources, perform specialised analytical functions (e.g. statistics, visualization, anomaly detection), and converge their outputs through a central agent which produces the final analytics report.</p> <p>Different architectures result in varying levels of system-wide risk, and these need to be considered carefully. Linear systems are vulnerable because tainted inputs spread from one agent to the next like a chain reaction, while centrally orchestrated systems can isolate failures to specific agents but risk having problems at the top level affect all subordinate agents.</p>"},{"location":"baseline/#roles-and-access-controls","title":"Roles and Access Controls","text":"<p>Roles and access controls establish differentiated responsibilities and permissions across agents within the system, ensuring that each agent operates within appropriate boundaries while being able to fulfill its designated function. This is critical because it limits unauthorised actions, contains the blast radius of potential failures or security breaches, and enables the system to maintain reliability even when individual agents may be compromised or behave unexpectedly.</p> <p>There are various possible roles depending on the level of specialisation required, such as orchestrator agents that manage workflows, specialist agents that perform pre-defined technical functions, or interface agents that handle external communications. Returning to our earlier example, some types of agents in a customer support system may include intake agents that receive and classify queries, knowledge agents that search documentation for answers, and response agents that craft and send customer communications. In contrast, an internal analytics system would employ different roles, such as data ingestion agents to collect information from various sources, analysis agents to generate insights, and reporting agents to create visualizations and reports.</p> <p>Defining roles and access controls poorly may result in agents having unauthorised access to data or systems, allowing compromised agents to delete files, steal information, or break critical systems. Additionally, agents may inherit too many privileges from shared accounts, meaning a simple data processing agent could accidentally cause major damage because it has access to powerful system controls it was never meant to use.</p>"},{"location":"baseline/#monitoring-and-traceability","title":"Monitoring and Traceability","text":"<p>Monitoring and traceability enable visibility into agentic system behaviour, interactions, and decision-making pathways, allowing developers and operators to understand what agents are doing, why they made particular choices, and how outcomes were produced. This capability is essential for post-hoc debugging, real-time anomaly detection, and establishing accountability particularly when agents operate with a degree of autonomy or interact with sensitive systems and data.</p> <p>Effective monitoring involves logging key agent actions, inputs and outputs, tool usage, and communication between agents or with external systems. Traceability goes a step further by linking these records together into coherent execution traces or decision trees that reflect the flow of information and control across the system. For example, in an automated loan approval system, traceability would reveal not just the final decision but also how the credit scoring agent, document parser, and fraud detection agent each contributed to it. This can be crucial for audits, regulatory compliance, or user explanations.</p> <p>Insufficient monitoring and traceability pose significant risks. Without visibility, it becomes difficult to detect when agents are malfunctioning, making biased or unsafe decisions, or deviating from their intended behaviour. It also creates blind spots where malicious activity can go unnoticed. In high-stakes domains, a lack of traceability can prevent incident investigation and undermine trust.</p> <ol> <li> <p>These are often described as levels of agency, see: https://developer.nvidia.com/blog/agentic-autonomy-levels-and-security/ or https://huggingface.co/blog/ethics-soc-7 \u21a9</p> </li> </ol>"},{"location":"baseline/controls/","title":"Baseline Controls","text":"<p>Here we provide a list of technical controls to mitigate the baseline risks discussed in the previous section. As our aim here is to provide sensible and concrete recommendations for teams, we do not include all potential mitigation measures here.</p> <p>Click here for a downloadable version.</p>"},{"location":"baseline/controls/#list-of-baseline-controls","title":"List of Baseline Controls","text":"Component Risk Control LLM Poorly aligned LLMs may pursue objectives which technically satisfy instructions but violate safety principles. Review the LLM's system card for potential alignment issues before using the LLM for more complex tasks. LLM Poorly aligned LLMs may pursue objectives which technically satisfy instructions but violate safety principles. Integrate an explicit safety constraint layer (e.g., policy engine or constitutional rules) that overrides unsafe outputs at runtime. LLM Poorly aligned LLMs may pursue objectives which technically satisfy instructions but violate safety principles. Maintain human\u2011in\u2011the\u2011loop approval for any high\u2011impact or irreversible actions. LLM Weaker LLMs have a higher tendency to produce unpredictable outputs which make agent behaviour erratic. Prioritise LLMs with stronger performance in instruction following and other related benchmarks. LLM Weaker LLMs have a higher tendency to produce unpredictable outputs which make agent behaviour erratic. Continuously monitor and log outputs, triggering alerts when behaviour drifts from tested baselines. LLM LLMs with poor safety tuning are more susceptible to prompt injection attacks and jailbreaking attempts. Implement input sanitisation measures or limit inputs to conventional ASCII characters only. LLM Using LLMs trained on poisoned or biased data introduces manipulation risk, discriminatory decisions, or misinformation. Do not use LLMs from unknown or untrusted sources, even if it is available on public platforms. Tools Poorly implemented tools may not correctly verify user identity or permissions when executing privileged actions. Do not use tools which do not implement robust authentication protocols. Tools Poorly implemented tools may not correctly verify user identity or permissions when executing privileged actions. Conduct periodic audits to validate that tool actions match the appropriate user permissions. Tools Rogue tools that mimic legitimate ones can contain hidden malicious code that executes when loaded. Do not use tools from unknown or untrusted sources, even if it is available on public platforms. Tools Rogue tools that mimic legitimate ones can contain hidden malicious code that executes when loaded. Test third\u2011party tools in hardened sandboxes with syscall/network egress restrictions before using them in production environments. Tools Tools that do not properly sanitise or validate inputs can be exploited through prompt injection attacks. Enforce strict schema validation (e.g. JSON Schema, protobuf) and reject non\u2011conforming inputs upstream. Tools Tools that do not properly sanitise or validate inputs can be exploited through prompt injection attacks. Escape or encode user inputs when embedding into tool prompts or commands. Tools Tools that demand broader permissions than necessary create unnecessary attack surfaces for malicious actors. Conduct periodic least\u2011privilege reviews and automated permission drift detection. Instructions Simplistic instructions with narrow metrics and without broader constraints may result in agents engaging in specification gaming, resulting in poor performance or safety violations. Define multi\u2011objective success criteria incorporating safety, ethics, and usability metrics. Instructions Simplistic instructions with narrow metrics and without broader constraints may result in agents engaging in specification gaming, resulting in poor performance or safety violations. Conduct adversarial evaluation to surface gaming behaviours and iterate on instruction design. Instructions Simplistic instructions with narrow metrics and without broader constraints may result in agents engaging in specification gaming, resulting in poor performance or safety violations. Continuously monitor and log agents' outputs, triggering alerts when behaviour drifts from tested baselines. Instructions Vague instructions may compel agents to attempt to fill in missing constraints, resulting in unpredictable actions or incorrect steps taken. Ask the agent to summarise its understanding and request clarification before proceeding. Instructions Vague instructions may compel agents to attempt to fill in missing constraints, resulting in unpredictable actions or incorrect steps taken. Test instructions with scenario\u2011based evaluations to reveal ambiguities for refinement. Instructions Instructions without a clear distinction between system prompts and user requests may confuse agents and result in greater vulnerability to prompt injection attacks. Signpost system prompts with clear tags (e.g. XML) to distinguish between system prompts and user inputs. Memory Malicious actors can inject false or misleading facts into the knowledge base, resulting in the agent acting on incorrect data or facts. Periodically run audits that reconcile stored facts against trusted external references, with a flag for discrepancies. Memory Agents may inadvertently store sensitive user or organisational data from prior interactions, resulting in data privacy risks. Encrypt memory at rest and restrict access via fine\u2011grained access controls and audit logs. Memory Agents may mistakenly save momentary glitches and hallucinations into memory, resulting in compounding mistakes when the agent relies on the incorrect information for its decision or actions. Schedule periodic memory reconciliation where human reviewers or external tools flag anomalies. Agentic Architecture In linear agentic pipelines where each stage blindly trusts the previous stage, single early mistakes may be propagated and magnified. Insert validation checkpoints between stages that verify assumptions and reject invalid outputs. Agentic Architecture In linear agentic pipelines where each stage blindly trusts the previous stage, single early mistakes may be propagated and magnified. Design feedback loops enabling later stages to roll back or request correction from earlier stages. Agentic Architecture In hub-and-spoke architectures which route all decisions through one controller agent, any bug or compromise may distributes faulty instructions across the entire system. Apply circuit\u2011breakers that freeze propagation when anomalous behaviour is detected. Agentic Architecture More complex agentic architectures may make it difficult to fully reconstruct decision processes across multiple agents. Implement end\u2011to\u2011end distributed tracing with unique request IDs across all agents and tool calls. Agentic Architecture More complex agentic architectures may make it difficult to fully reconstruct decision processes across multiple agents. Write immutable, tamper\u2011evident audit logs that capture prompts, responses, and tool invocations. Roles and Access Controls Unauthorised actors can impersonate agents and gain access to restricted resources. Maintain trusted registry of agents and authenticate agents using strong, verifiable credentials. Roles and Access Controls Agents may gain unauthorized access to restricted resources by exploiting misconfigured or overly permissive roles. Apply Principle of Least Privilege (PoLP) when configuring all agent and delegation roles. Roles and Access Controls Agents may gain unauthorized access to restricted resources by exploiting misconfigured or overly permissive roles. Apply strict access controls and validate agent roles for requests. Roles and Access Controls Agents may gain unauthorized access to restricted resources by exploiting misconfigured or overly permissive roles. Ensure fine-grained, scoped tokens or credentials where possible. Roles and Access Controls Agents may gain unauthorized access to restricted resources by exploiting misconfigured or overly permissive roles. Use time-bound or one-time-use credentials where possible. Monitoring and Traceability Lack of monitoring results in delayed detection of agent failures. Implement real-time monitoring of agent status, actions, and performance metrics, paired with automated alerting mechanisms that notify operators of anomalies, errors, or inactivity. Monitoring and Traceability Lack of traceability inhibit proper audit of decision-making paths in the event of failures. Record comprehensive logs of agent actions, inputs, outputs, and inter-agent communications, tagged with unique trace identifiers to reconstruct full decision-making paths."},{"location":"baseline/risks/","title":"Baseline Risks","text":"<p>In this section, we list the baseline risks from (i) the components of an agent and (ii) the design of an agentic system. Note that this list is meant as a reference and is not meant to be exhaustive. Clicking on any of the risks will bring you to the next page with the corresponding controls.</p> <p>Click here for a downloadable version.</p>"},{"location":"baseline/risks/#list-of-baseline-risks","title":"List of Baseline Risks","text":"Component Risk LLM Poorly aligned LLMs may pursue objectives which technically satisfy instructions but violate safety principles.<sup>1</sup> LLM Weaker LLMs have a higher tendency to produce unpredictable outputs which make agent behaviour erratic.<sup>2</sup> LLM LLMs with poor safety tuning are more susceptible to prompt injection attacks and jailbreaking attempts.<sup>3</sup> LLM Using LLMs trained on poisoned or biased data introduces manipulation risk, discriminatory decisions, or misinformation.<sup>4</sup> Tools Poorly implemented tools may not correctly verify user identity or permissions when executing privileged actions.<sup>5</sup> Tools Rogue tools that mimic legitimate ones can contain hidden malicious code that executes when loaded.<sup>6</sup> Tools Tools that do not properly sanitise or validate inputs can be exploited through prompt injection attacks.<sup>7</sup> Tools Tools that demand broader permissions than necessary create unnecessary attack surfaces for malicious actors.<sup>8</sup> Instructions Simplistic instructions with narrow metrics and without broader constraints may result in agents engaging in specification gaming, resulting in poor performance or safety violations.<sup>9</sup> Instructions Vague instructions may compel agents to attempt to fill in missing constraints, resulting in unpredictable actions or incorrect steps taken.<sup>10</sup> Instructions Instructions without a clear distinction between system prompts and user requests may confuse agents and result in greater vulnerability to prompt injection attacks.<sup>11</sup> Memory Malicious actors can inject false or misleading facts into the knowledge base, resulting in the agent acting on incorrect data or facts.<sup>12</sup> Memory Agents may inadvertently store sensitive user or organisational data from prior interactions, resulting in data privacy risks.<sup>13</sup> Memory Agents may mistakenly save momentary glitches and hallucinations into memory, resulting in compounding mistakes when the agent relies on the incorrect information for its decision or actions. Agentic Architecture In linear agentic pipelines where each stage blindly trusts the previous stage, single early mistakes may be propagated and magnified.<sup>14</sup> Agentic Architecture In hub-and-spoke architectures which route all decisions through one controller agent, any bug or compromise may distributes faulty instructions across the entire system.<sup>15</sup> Agentic Architecture More complex agentic architectures may make it difficult to fully reconstruct decision processes across multiple agents. Roles and Access Controls Unauthorised actors can impersonate agents and gain access to restricted resources.<sup>16</sup> Roles and Access Controls Agents may gain unauthorized access to restricted resources by exploiting misconfigured or overly permissive roles.<sup>17</sup> Monitoring and Traceability Lack of monitoring results in delayed detection of agent failures and downstream risks.<sup>18</sup> Monitoring and Traceability Lack of traceability inhibit proper audit of decision-making paths in the event of failures.<sup>17</sup> <ol> <li> <p>Denison et al. Sycophancy to Subterfuge: Investigating Reward-Tampering in Large Language Models. https://arxiv.org/abs/2406.10162, 2024. Accessed: 2025-07-21.\u00a0\u21a9</p> </li> <li> <p>Zhang et al. Which Agent Causes Task Failures and When? On Automated Failure Attribution of LLM Multi-Agent Systems. https://arxiv.org/abs/2505.00212, 2025. Accessed: 2025-07-21.\u00a0\u21a9</p> </li> <li> <p>See Commercial LLM Agents Are Already Vulnerable to Simple Yet Dangerous Attacks (Li et al, 2025) and Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents (Yang et al, 2024).\u00a0\u21a9</p> </li> <li> <p>Bowen et al. Scaling Trends for Data Poisoning in LLMs. https://arxiv.org/abs/2408.02946v6, 2024. Accessed: 2025-07-21.\u00a0\u21a9</p> </li> <li> <p>See Enterprise-Grade Security for the Model Context Protocol (MCP): Frameworks and Mitigation Strategies (Vineeth Sai Narajala and Idan Habler, 2025), MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol (Jing et al, 2025), and Model Context Protocol (MCP): Understanding security risks and controls (Florencio Cano Gabarda, 2025) \u21a9</p> </li> <li> <p>Bargury, Michael. MCP: Untrusted Servers and Confused Clients, Plus a Sneaky Exploit. https://www.mbgsec.com/archive/2025-05-03-mcp-untrusted-servers-and-confused-clients-plus-a-sneaky-exploit-embrace-the-red/, 2025. Accessed: 2025-07-21.\u00a0\u21a9</p> </li> <li> <p>See Multi-Agent Systems Execute Arbitrary Malicious Code (Triedman et al, 2025) and CVE-2024-7042.\u00a0\u21a9</p> </li> <li> <p>Rehberger, Johann. Plugin Vulnerabilities: Visit a Website and Have Your Source Code Stolen. https://embracethered.com/blog/posts/2023/chatgpt-plugin-vulns-chat-with-code/, 2023. Accessed: 2025-07-21.\u00a0\u21a9</p> </li> <li> <p>Bondarenko et  al. Demonstrating specification gaming in reasoning models. https://arxiv.org/abs/2502.13295, 2025. Accessed: 2025-07-21.\u00a0\u21a9</p> </li> <li> <p>In What Prompts Don't Say: Understanding and Managing Underspecification in LLM Prompts, Yang et al (2025) show that underspecified prompts are two times more likely to regress over model or prompt changes, with accuracy drops exceeding 20% in some cases.\u00a0\u21a9</p> </li> <li> <p>See Control Illusion: The Failure of Instruction Hierarchies in Large Language Models (Geng et al, 2025) and IHEval: Evaluating Language Models on Following the Instruction Hierarchy (Zhang et al, 2025).\u00a0\u21a9</p> </li> <li> <p>See One Shot Dominance: Knowledge Poisoning Attack on Retrieval-Augmented Generation Systems (Chang et al, 2025) and PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models (Zou et al, 2025) for text-based knowledge bases; see PoisonedEye: Knowledge Poisoning Attack on Retrieval-Augmented Generation based Large Vision-Language Models (Zhang et al, 2025) for image-based knowledge bases.\u00a0\u21a9</p> </li> <li> <p>Shanmugarasa et al. SoK: The Privacy Paradox of Large Language Models: Advancements, Privacy Risks, and Mitigation. https://arxiv.org/abs/2506.12699v2, 2025. Accessed: 2025-07-21.\u00a0\u21a9</p> </li> <li> <p>Huang et al. On the Resilience of LLM-Based Multi-Agent Collaboration with Faulty Agents. https://arxiv.org/abs/2408.00989v3, 2025. Accessed: 2025-07-21.\u00a0\u21a9</p> </li> <li> <p>Peign\u00e9-Lefebvre et al. Multi-Agent Security Tax: Trading Off Security and Collaboration Capabilities in Multi-Agent Systems. arXiv preprint arXiv:2502.19145, 2025. https://arxiv.org/pdf/2502.19145, Accessed: 2025\u201107\u201127.\u00a0\u21a9</p> </li> <li> <p>Chen et al. AI Agents Are Here. So Are the Threats. Palo Alto Networks Unit\u202f42 blog, May 1 2025. https://unit42.paloaltonetworks.com/agentic-ai-threats/, Accessed: 2025\u201107\u201127.\u00a0\u21a9</p> </li> <li> <p>Goutham A S. Escaping Reality: Privilege Escalation in Gen AI Admin Panel (aka The Chaos of a Misconfigured Admin Panel). Medium blog, Sept 23\u202f2024. https://cyberweapons.medium.com/escaping-reality-privilege-escalation-in-gen-ai-admin-panel-aka-the-chaos-of-a-misconfigured-b6ad73bf1b65, Accessed: 2025\u201107\u201127.\u00a0\u21a9\u21a9</p> </li> <li> <p>Chan et al. Visibility into AI Agents. ACM FAccT 2024, 2024. https://arxiv.org/abs/2401.13138, Accessed: 2025-08-04.\u00a0\u21a9</p> </li> </ol>"},{"location":"capability/","title":"Capabilities","text":"<p>In this section, we describe the different capabilities that agentic AI systems may have. We start with three broad categories of capabilities - cognitive, interaction, and operational - and break it down into more granular capabilities.</p> <p>Click here for a downloadable version.</p>"},{"location":"capability/#cognitive-capabilities","title":"Cognitive Capabilities","text":"<p>Cognitive capabilities encompass the agentic AI system's internal \"thinking\" skills \u2013 how it analyses information, forms plans, learns from experience, and monitors its own performance.</p> <ol> <li> <p>Reasoning &amp; Problem-Solving: The capability to perform structured, multi-step reasoning that demonstrates deeper understanding, problem-solving, and decision-making. This includes deconstructing complex prompts, making inferences, and drawing conclusions from available information. An example would be analysing a customer complaint by first identifying the core issue, then considering possible causes, and finally proposing a solution based on company policies.</p> </li> <li> <p>Planning &amp; Goal Management: The capability to develop detailed, step-by-step, and executable plans with specific tasks in response to broad instructions. This includes prioritising activities based on importance and dependencies between tasks, monitoring how well its plan is working, and adjusting when circumstances change or obstacles arise. For instance, when asked to \"organise a team meeting,\" the agentic AI system should be able to break this down into steps like checking calendars, booking a room, sending invites, and preparing an agenda, as well as automatically finding alternative timeslots and updating the details if a required participant becomes unavailable.</p> </li> <li> <p>Agent Delegation: The capability to assign subtasks to other agents and coordinate their activities to achieve broader goals. This includes identifying which components are best suited for specific tasks, issuing clear instructions, managing inter-agent dependencies, and monitoring performance or failures. For example, an agentic AI system tasked with generating a market research report may delegate data collection to a web-scraping agent, assign analysis to a statistical reasoning agent, and pass the results to a report generation agent to draft the final document, while coordinating timing and verifying outputs from each.</p> </li> <li> <p>Tool Use: The capability to evaluate available options and choose the best tool for specific subtasks. This requires agents to understand the capabilities and limitations of different tools, match them appropriately to the tasks, check that the work completed is satisfactory (and requesting corrections when not), and combine multiple outputs into a coherent final result. For example, when processing a document, an agentic system might choose an OCR tool for scanned images but a direct text parser for digital PDFs.</p> </li> </ol>"},{"location":"capability/#interaction-capabilities","title":"Interaction Capabilities","text":"<p>Interaction capabilities describe how the agentic AI system exchanges information with users, other agents, and external systems. These capabilities below are broadly differentiated based on how and what they interact with. </p> <ol> <li> <p>Natural Language Communication: The capability to fluently and meaningfully converse with human users, handling a wide range of situations such as explaining complex topics, generating documents or prose, or discussing issues with human users. It produces natural-sounding text which matches the appropriate tone, formality level, and communication style for the situation. For instance, an agentic knowledge bot should be able to adjust its response and tone depending on the content and nuances of the user's query and provide a tailored and sensitive response.</p> </li> <li> <p>Multimodal Understanding &amp; Generation: The capability to to take in image, audio, or video inputs and / or generate image, audio, or video outputs. This includes analysing visual information, transcribing speech, or creating multimedia content as needed. As an example, it might analyze a chart in an uploaded image to extract data trends, or generate a diagram to illustrate a complex process being discussed.</p> </li> <li> <p>Official Communication: The capability to compose and directly publish communications that formally represent an organisation to external parties (e.g., customers, partners, regulators, courts, media) via approved channels and formats without human oversight or approval. For instance, an AI customer service agent may be tasked to directly respond to simple queries coming into the company's customer service portal without requiring a human operator to review the response.</p> </li> <li> <p>Business Transactions: The capability to execute transactions that involve exchanging money, services, or commitments with external parties. It can process payments, make reservations, and handle other business transactions within authorized limits. For instance, it might book travel arrangements for employees, process refunds for customers, or automatically pay recurring vendor invoices.</p> </li> <li> <p>Internet &amp; Search Access: The capability to access and search the Internet for services or resources, especially for up-to-date information to supplement its knowledge and provide more accurate answers. For example, an agentic market research system, when asked about recent market trends, could search financial news sites and compile the latest relevant information. Note that this capability specifically refers to external internet resources, not accessing private organisational documents or internal systems.</p> </li> <li> <p>Computer Use: The capability to directly control a computer interface by moving the mouse, clicking buttons, and typing on behalf of the user. It can navigate applications and perform tasks that require interacting with graphical user interfaces. For example, it might automatically fill out online forms, navigate through software menus to change settings, or perform repetitive data entry tasks.</p> </li> <li> <p>Other Programmatic Interfaces: The capability to interact with external systems through APIs, SDKs, or backend services. This includes sending and receiving data via RESTful APIs, pushing code to a remote repository, or invoking cloud services to retrieve or manipulate information from other systems. For instance, a coding assistant agent may call GitHub\u2019s API to fetch issue tickets, query a documentation API to retrieve function specifications, or push commits to a version control system as part of an automated development workflow.</p> </li> </ol>"},{"location":"capability/#operational-capabilities","title":"Operational Capabilities","text":"<p>Operational capabilities focus on the agentic AI system's ability to execute actions safely and efficiently within its operating environment.</p> <ol> <li> <p>Agent Communication: The capability to communicate with other agents within the system, either through natural language or a predefined protocol, and to coordinate with other agents to accomplish complex tasks that require multiple specialties. This would include sending instructions, sharing information, and coordinating workflows with other agents in the system. For example, a customer service agent may communicate with a billing specialist agent to resolve a payment dispute, sharing relevant account details and receiving updated billing information.</p> </li> <li> <p>Code Execution: The capability to write, execute, and debug code in various programming languages to automate tasks or solve computational problems. It can work with different programming languages and understand code outputs to determine if programs ran successfully. For example, it might write a Python script to analyze a dataset and then execute it to generate insights and visualizations.</p> </li> <li> <p>File &amp; Data Management: The capability to create, read, modify, organise, convert, query, and update information across both unstructured files (e.g., PDFs, Word docs, spreadsheets) and structured data stores (e.g., SQL/NoSQL databases, data warehouses, vector stores). For example, it can find and merge documents, extract and transform data in CSV files, write and execute database queries, or move files to different locations. </p> </li> <li> <p>System Management: The capability to adjust system configurations, manage computing resources, and handle technical infrastructure tasks. This includes monitoring system performance, securely handle authentication information and access controls, and making optimizations as needed while maintaining security best practices. For instance, it might allocate additional memory to a slow-running process or configure network settings for a new application, and it might rotate API keys on a schedule or grant temporary access permissions to team members for specific projects.</p> </li> </ol>"},{"location":"capability/controls/","title":"Capability Controls","text":"<p>For each capability and risk combination identified in the previous sections, we provide a list of technical controls that may be useful in mitigating those safety or security risks. Due to the rapid developments in the agentic AI space, this list provides a simple starting point and is far from complete or thorough.</p> <p>Click here for a downloadable version.</p> Category Capability Risk Technical Control Cognitive Planning &amp; Goal Management Devising plans that are not effective in meeting the user's requirements Prompt the agent to self-reflect on the adherence of the plan to the user's instructions Cognitive Planning &amp; Goal Management Devising plans that are not effective in meeting the user's requirements Require the user to approve the plan in high-impact cases Cognitive Planning &amp; Goal Management Devising plans that do not adhere to common sense or implicit assumptions about the user's instructions Prompt the agent to self-reflect on whether the plan is sensible and reasonable, given the user's original request Cognitive Planning &amp; Goal Management Devising plans that do not adhere to common sense or implicit assumptions about the user's instructions Ensure important assumptions about feasibility, scope, and cost, where relevant, are included in the system prompt Cognitive Reasoning &amp; Problem-Solving Becoming ineffective, inefficient, or unsafe due to overthinking Enforce time or token limits for agents' reasoning Cognitive Reasoning &amp; Problem-Solving Becoming ineffective, inefficient, or unsafe due to overthinking Test different variations of the prompt to estimate likelihood of overthinking Cognitive Reasoning &amp; Problem-Solving Becoming ineffective, inefficient, or unsafe due to overthinking Adjust short-term and long-term memory options for the agent Cognitive Reasoning &amp; Problem-Solving Engaging in deceptive behaviour through pursuing or prioritising other goals Provide access to a scratchpad for agents to use to record its inner thoughts Cognitive Agent Delegation Assigning tasks incorrectly to other agents Apply guardrails to limit the scope of tasks that can be assigned to specialised agents Cognitive Agent Delegation Attempting to use other agents maliciously Log all task assignments by the agent to other agents Cognitive Agent Delegation Attempting to use other agents maliciously Conduct rigorous adversarial testing on centralised planning agents Cognitive Tool Use Choosing the wrong tool for the given action or task Provide comprehensive descriptions of each tool, including its intended use, required inputs, and potential outputs Interaction Natural Language Communication Generating undesirable content (e.g. toxic, hateful, sexual) Implement output safety text guardrails to detect if undesirable content is being generated Interaction Natural Language Communication Generating unqualified advice in specialised domains (e.g. medical, financial, legal) Implement input text guardrails to detect if the question is related to one of the specialised domains, and if so, to decline answering the question Interaction Natural Language Communication Generating controversial content (e.g. political, competitors) Implement input text guardrails to detect instructions to generate content that is controversial according to the organisation's policies Interaction Natural Language Communication Regurgitating personally identifiable information Implement output text guardrails to detect personally identifiable information in the LLM's outputs before it reaches the user Interaction Natural Language Communication Generating non-factual or hallucinated content Implement methods to reduce hallucination rates (e.g. retrieval-augmented generation) Interaction Natural Language Communication Generating non-factual or hallucinated content Implement UI/UX cues to highlight the risk of hallucination to the user Interaction Natural Language Communication Generating non-factual or hallucinated content Implement features to enable users to easily verify the generated answer against the original content Interaction Natural Language Communication Generating copyrighted content Implement input text guardrails to detect instructions to generate copyrighted content Interaction Multimodal Understanding &amp; Generation Generating undesirable content (e.g. toxic, hateful, sexual) Implement output multimodal safety guardrails for the output to detect if undesirable content is being generated Interaction Multimodal Understanding &amp; Generation Generating unqualified advice in specialised domains (e.g. medical, financial, legal) Implement input multimodal guardrails to detect if the instruction is related to one of the specialised domains, and if so, to decline fulfilling the instruction Interaction Multimodal Understanding &amp; Generation Generating controversial content (e.g. political, competitors) Implement input multimodal guardrails to detect instructions to generate content that is controversial according to the organisation's policies Interaction Multimodal Understanding &amp; Generation Regurgitating personally identifiable information Implement output multimodal guardrails to detect personally identifiable information in the LLM's outputs before it reaches the user Interaction Multimodal Understanding &amp; Generation Generating non-factual or hallucinated content Conduct testing to measure hallucination and factuality rates for multimodal outputs Interaction Multimodal Understanding &amp; Generation Generating copyrighted content Implement input guardrails to detect instructions to generate copyrighted content Interaction Official Communications Making inaccurate promises or statements to the public Limit the communications to standard processes, where communication templates are available Interaction Official Communications Making inaccurate promises or statements to the public Require human approval for communications for more sensitive matters Interaction Official Communications Making inaccurate promises or statements to the public Provide alternate channels for users to clarify communications or give feedback Interaction Official Communications Sending undesirable content to recipients Implement output safety guardrails to detect if undesirable content is in the communications before it is sent to the user Interaction Official Communications Sending malicious content to recipients Check for adherence to communication templates prior to sending email Interaction Official Communications Sending malicious content to recipients Validate all links and attachments prior to sending them to users Interaction Official Communications Misleading recipients about the authorship of the communications Declare upfront that the communications are generated by an AI system Interaction Official Communications Sending personally identifiable or sensitive data Implement output guardrails to detect personally identifiable information in the communications before it is sent to the user Interaction Business Transactions Allowing unauthorised transactions Require human validation for high-impact transactions Interaction Business Transactions Allowing unauthorised transactions Logging all requests leading up to the transaction Interaction Business Transactions Allowing unauthorised transactions Apply fraud detection models or heuristics to the agent's own decisions Interaction Business Transactions Increasing the system's vulnerability to attackers exfiltrating credentials for transactions through the agent Ensure virtual isolation for agents carrying out transactions Interaction Business Transactions Increasing the system's vulnerability to attackers exfiltrating credentials for transactions through the agent Do not share credentials with the agent directly, require the agent to use a separate service for authentication and transactions Interaction Internet &amp; Search Access Opening vulnerabilities to prompt injection attacks via malicious websites Implement input guardrails to detect prompt injection or adversarial attacks Interaction Internet &amp; Search Access Opening vulnerabilities to prompt injection attacks via malicious websites Implement escape filtering before including web content into prompts Interaction Internet &amp; Search Access Opening vulnerabilities to prompt injection attacks via malicious websites Use structured retrieval APIs for searching the web rather than through web scraping Interaction Internet &amp; Search Access Returning unreliable information or websites Prioritise results from verified, high-quality domains (e.g. .gov, .edu, well-known publishers) Interaction Internet &amp; Search Access Returning unreliable information or websites Require cross-source validation for some of the claims made Interaction Computer Use Opening vulnerabilities to prompt injection attacks Ensure computer use protocol or application provides immediate interruptability Interaction Computer Use Opening vulnerabilities to prompt injection attacks Limit computer use to accessing only safe resources on the computer Interaction Computer Use Accessing personally identifiable or sensitive data Ensure \"take over\" mode is activated when keying in sensitive data (e.g. passwords, API keys) Interaction Other Programmatic Interfaces Leaking personally identifiable or sensitive data Use short\u2011lived, rotating credentials that expire immediately after agent use Interaction Other Programmatic Interfaces Leaking personally identifiable or sensitive data Specify a whitelist of interfaces that agents are allowed to use Interaction Other Programmatic Interfaces Increasing the system's vulnerability to supply chain attacks Enforce zero-trust input handling and validate all data flows Operational Agent Communication Enabling the exfiltration of sensitive data Implement a whitelist approach for outward network access, including API requests Operational Agent Communication Enabling the exfiltration of sensitive data Ensure that sensitive data is not passed and leaked between agents by using appropriate guardrails Operational Agent Communication Communicating insecurely resulting in man-in-the-middle attacks Ensure all cross-agent authentication and message validation and encryption where necessary Operational Agent Communication Misinterpreting inter-agent messages due to poor formatting or weak protocols Constrain agent communication with structured outputs and interactions Operational Agent Communication Passing on prompt injection attacks across agents Sanitise messages before agents process them - strip or escape unexpected instruction-like content that may have been injected Operational Agent Communication Impersonating or accessing peer agents or services via shared roles or credentials Isolate roles and credentials of each agent Operational Code Execution Executing poor code Use code linters to screen for bad practices, anti-patterns, unused variables, or poor syntax Operational Code Execution Executing poor code Use static code analysers to detect problems with the code Operational Code Execution Executing poor code Run code only in virtually isolated compute environments (e.g. Docker containers) Operational Code Execution Executing poor code Ensure monitoring of code runtime and memory consumption Operational Code Execution Executing vulnerable or malicious code Use static code analysers to identify dangerous patterns in the code before execution Operational Code Execution Executing vulnerable or malicious code Conduct CVE scanning and block execution if any High or Critical CVEs are detected Operational Code Execution Executing vulnerable or malicious code Block all inward and outward network access by default Operational Code Execution Executing vulnerable or malicious code Scope execution privileges strictly only to what is necessary, ensuring that privileges are customised to each agent within a system Operational Code Execution Executing vulnerable or malicious code Do not grant admin or sudo privileges Operational Code Execution Executing vulnerable or malicious code Sanitise all inputs Operational Code Execution Executing vulnerable or malicious code Implement a whitelist approach for inward network access Operational Code Execution Executing vulnerable or malicious code Review all code generated by agents, including shell scripts, before execution Operational Code Execution Executing vulnerable or malicious code Create a Deny list of commands that agents are not allowed to run autonomously Operational File &amp; Data Management Overwriting or deleting database tables or files No write access to tables in the database unless strictly required Operational File &amp; Data Management Overwriting or deleting database tables or files Require human approval for any changes to the database, table, or file Operational File &amp; Data Management Overwriting or deleting database tables or files Avoid mounting broad or persistant paths Operational File &amp; Data Management Overwhelming the database with poor, inefficient, or repeated queries Limit the number of concurrent queries to the database from the agent Operational File &amp; Data Management Overwhelming the database with poor, inefficient, or repeated queries Analyse past database queries to identify repeated or inefficient queries Operational File &amp; Data Management Exposing personally identifiable or sensitive data from databases or files Implement input guardrails to detect personally identifiable information Operational File &amp; Data Management Exposing personally identifiable or sensitive data from databases or files Do not allow access to personally identifiable data or sensitive data unless strictly required Operational File &amp; Data Management Exposing personally identifiable or sensitive data from databases or files Log all database queries in production Operational File &amp; Data Management Opening vulnerabilities to prompt injection attacks via malicious data or files Validate new data used to supplement RAG databases or training data Operational File &amp; Data Management Opening vulnerabilities to prompt injection attacks via malicious data or files Implement input guardrails to detect prompt injection or adversarial attacks Operational File &amp; Data Management Opening vulnerabilities to prompt injection attacks via malicious data or files Disallow unknown or external files unless it is scanned Operational File &amp; Data Management Overwriting or deleting required files Require user confirmation before overwriting or deleting any files Operational File &amp; Data Management Overwriting or deleting required files Keep separate copy of original files Operational File &amp; Data Management Overwriting or deleting required files Ensure second copy of database is not changed until a pre-specified amount of time has passed / ensure database versioning Operational File &amp; Data Management Making unauthorised changes to files Require user confirmation before executing each change to a file Operational System Management Escalating the agent's own privileges Scope system privileges strictly only to what is necessary Operational System Management Escalating the agent's own privileges Do not grant admin privileges to agents Operational System Management Escalating the agent's own privileges Do not allow agents to modify privileges Operational System Management Misconfiguring system resources, compromising system integrity and availability Only grant agents privileges to modify system resources if strictly necessary for completion of tasks Operational System Management Misconfiguring system resources, compromising system integrity and availability Set minimum and maximum limits to what the agent can modify within a given system resource Operational System Management Misconfiguring system resources, compromising system integrity and availability Ensure logging of system health metrics and automated alerts to the developer team if any metrics are abnormal Operational System Management Overwhelming the system with poor, inefficient, or repeated requests Limit the number of concurrent queries to external systems from the agent Operational System Management Overwhelming the system with poor, inefficient, or repeated requests Log all queries to external systems from the agent"},{"location":"capability/risks/","title":"Capability Risks","text":"<p>Similar to what we did for baseline components and risks, we identify safety and security risks that may arise from the specific capabilities identified in the previous section. For each risk, we provide an academic paper, case study, or article which provides more details about the nature of the risk.</p> <p>Click here for a downloadable version.</p> Category Capability Risk Cognitive Reasoning &amp; Problem-Solving Becoming ineffective, inefficient, or unsafe due to overthinking<sup>1</sup> Cognitive Reasoning &amp; Problem-Solving Engaging in deceptive behaviour through pursuing or prioritising other goals<sup>2</sup> Cognitive Planning &amp; Goal Management Devising plans that are not effective in meeting the user's requirements<sup>3</sup> Cognitive Planning &amp; Goal Management Devising plans that do not adhere to common sense or implicit assumptions about the user's instructions<sup>4</sup> Cognitive Agent Delegation Assigning tasks incorrectly to other agents<sup>5</sup> Cognitive Agent Delegation Attempting to use other agents maliciously<sup>6</sup> Cognitive Tool Use Choosing the wrong tool for the given action or task<sup>7</sup> Interaction Natural Language Communication Generating undesirable content (e.g. toxic, hateful, sexual)<sup>8</sup> Interaction Natural Language Communication Generating unqualified advice in specialised domains (e.g. medical, financial, legal)<sup>9</sup> Interaction Natural Language Communication Generating controversial content (e.g. political, competitors)<sup>10</sup> Interaction Natural Language Communication Regurgitating personally identifiable information<sup>11</sup> Interaction Natural Language Communication Generating non-factual or hallucinated content<sup>12</sup> Interaction Natural Language Communication Generating copyrighted content<sup>13</sup> Interaction Multimodal Understanding &amp; Generation Generating undesirable content (e.g. toxic, hateful, sexual)<sup>14</sup> Interaction Multimodal Understanding &amp; Generation Generating unqualified advice in specialised domains (e.g. medical, financial, legal)<sup>15</sup> Interaction Multimodal Understanding &amp; Generation Generating controversial content (e.g. political, competitors)<sup>16</sup> Interaction Multimodal Understanding &amp; Generation Regurgitating personally identifiable information<sup>17</sup> Interaction Multimodal Understanding &amp; Generation Generating non-factual or hallucinated content<sup>18</sup> Interaction Multimodal Understanding &amp; Generation Generating copyrighted content<sup>19</sup> Interaction Official Communications Making inaccurate promises or statements to the public<sup>20</sup> Interaction Official Communications Sending undesirable content to recipients<sup>21</sup> Interaction Official Communications Sending malicious content to recipients<sup>22</sup> Interaction Official Communications Misleading recipients about the authorship of the communications<sup>23</sup> Interaction Official Communications Sending personally identifiable or sensitive data<sup>24</sup> Interaction Business Transactions Allowing unauthorised transactions<sup>25</sup> Interaction Business Transactions Increasing the system's vulnerability to attackers exfiltrating credentials for transactions through the agent<sup>26</sup> Interaction Internet &amp; Search Access Opening vulnerabilities to prompt injection attacks via malicious websites<sup>27</sup> Interaction Internet &amp; Search Access Returning unreliable information or websites<sup>28</sup> Interaction Computer Use Opening vulnerabilities to prompt injection attacks<sup>29</sup> Interaction Computer Use Accessing personally identifiable or sensitive data<sup>30</sup> Interaction Other Programmatic Interfaces Leaking personally identifiable or sensitive data<sup>31</sup> Interaction Other Programmatic Interfaces Increasing the system's vulnerability to supply chain attacks<sup>32</sup> Operational Agent Communication Enabling the exfiltration of sensitive data<sup>33</sup> Operational Agent Communication Communicating insecurely resulting in man-in-the-middle attacks<sup>34</sup> Operational Agent Communication Misinterpreting inter-agent messages due to poor formatting or weak protocols<sup>35</sup> Operational Agent Communication Passing on prompt injection attacks across agents<sup>36</sup> Operational Code Execution Executing poor code<sup>37</sup> Operational Code Execution Executing vulnerable or malicious code<sup>38</sup> Operational File &amp; Data Management Overwriting or deleting database tables or files<sup>39</sup> Operational File &amp; Data Management Overwhelming the database with poor, inefficient, or repeated queries<sup>40</sup> Operational File &amp; Data Management Exposing personally identifiable or sensitive data from databases or files<sup>41</sup> Operational File &amp; Data Management Opening vulnerabilities to prompt injection attacks via malicious data or files<sup>42</sup> Operational System Management Escalating the agent's own privileges<sup>43</sup> Operational System Management Misconfiguring system resources, compromising system integrity and availability<sup>44</sup> Operational System Management Overwhelming the system with poor, inefficient, or repeated requests<sup>45</sup> <ol> <li> <p>Cuadron et al. The Danger of Overthinking: Examining the Reasoning\u2011Action Dilemma in Agentic Tasks. arXiv preprint arXiv:2502.08235, 2025. https://arxiv.org/pdf/2502.08235, Accessed: 2025\u201107\u201126.\u00a0\u21a9</p> </li> <li> <p>Chen et al. Reasoning Models Don't Always Say What They Think. https://www.anthropic.com/research/reasoning-models-dont-say-think, Accessed: 2025\u201107\u201126.\u00a0\u21a9</p> </li> <li> <p>Xie et al. TravelPlanner: A Benchmark for Real\u2011World Planning with Language Agents. arXiv preprint arXiv:2402.01622, 2024. https://arxiv.org/pdf/2402.01622v4, Accessed: 2025\u201107\u201126. Xie et al. Revealing the Barriers of Language Agents in Planning. NAACL Long Papers 2025. https://aclanthology.org/2025.naacl-long.93.pdf, Accessed: 2025\u201107\u201126.\u00a0\u21a9</p> </li> <li> <p>Marcus et al. AI still lacks \"common\" sense, 70 years later. Substack essay, January 5, 2025. https://garymarcus.substack.com/p/ai-still-lacks-common-sense-70-years, Accessed: 2025\u201107\u201126.\u00a0\u21a9</p> </li> <li> <p>Cemri et al. Why Do Multi\u2011Agent LLM Systems Fail? arXiv preprint arXiv:2503.13657, 2025. https://arxiv.org/abs/2503.13657, Accessed: 2025\u201107\u201126.\u00a0\u21a9</p> </li> <li> <p>Lupinacci et al. The Dark Side of LLMs: Agent\u2011based Attacks for Complete Computer Takeover. arXiv preprint arXiv:2507.06850, 2025. https://arxiv.org/abs/2507.06850, Accessed: 2025\u201107\u201126.\u00a0\u21a9</p> </li> <li> <p>Kokane et al. ToolScan: A Benchmark for Characterizing Errors in Tool\u2011Use LLMs. arXiv preprint arXiv:2411.13547, 2024. https://arxiv.org/abs/2411.13547, Accessed: 2025\u201107\u201126.\u00a0\u21a9</p> </li> <li> <p>Mazeika et al. HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal. arXiv preprint arXiv:2402.04249v2, 2024. https://arxiv.org/abs/2402.04249v2, Accessed: 2025\u201107\u201126.\u00a0\u21a9</p> </li> <li> <p>Barbera, Isabel. AI Privacy Risks &amp; Mitigations in Large Language Models (LLMs). European Data Protection Board Report, 2025. https://www.edpb.europa.eu/system/files/2025-04/ai-privacy-risks-and-mitigations-in-llms.pdf, Accessed: 2025\u201107\u201126.\u00a0\u21a9</p> </li> <li> <p>Stanford HAI. AI models like ChatGPT, Claude, and Gemini show partisan bias, study finds. Stanford News, May 22, 2025. https://news.stanford.edu/stories/2025/05/ai-models-llms-chatgpt-claude-gemini-partisan-bias-research-study, Accessed: 2025\u201107\u201126.\u00a0\u21a9</p> </li> <li> <p>See footnote 9.\u00a0\u21a9</p> </li> <li> <p>Zhang et al. Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models. arXiv  preprint arXiv:2309.01219, 2023. https://arxiv.org/abs/2309.01219, Accessed: 2025\u201107\u201126.\u00a0\u21a9</p> </li> <li> <p>Chen et al. CopyBench: Measuring Literal and Non-Literal Reproduction of Copyright\u2011Protected Text in Language Model Generation. arXiv preprint arXiv:2407.07087, 2024. https://arxiv.org/abs/2407.07087, Accessed: 2025\u201107\u201126.\u00a0\u21a9</p> </li> <li> <p>Liu et al. MM\u2011SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models. arXiv preprint arXiv:2311.17600, 2023. https://arxiv.org/abs/2311.17600, Accessed: 2025\u201107\u201126.\u00a0\u21a9</p> </li> <li> <p>Yan et al. Worse than Random? An Embarrassingly Simple Probing Evaluation of Large Multimodal Models in Medical VQA. Findings of ACL 2025. https://aclanthology.org/2025.findings-acl.981.pdf, Accessed: 2025\u201107\u201127.\u00a0\u21a9</p> </li> <li> <p>Motoki et al. Assessing political bias and value misalignment in generative artificial intelligence. Journal of Economic Behavior &amp; Organization, 2025. https://www.sciencedirect.com/science/article/pii/S0167268125000241, Accessed: 2025\u201107\u201127.\u00a0\u21a9</p> </li> <li> <p>Carlini et al. Extracting Training Data from Diffusion Models. arXiv preprint arXiv:2301.13188, 2023. https://arxiv.org/abs/2301.13188, Accessed: 2025\u201107\u201127.\u00a0\u21a9</p> </li> <li> <p>Bai et al. Hallucination of Multimodal Large Language Models: A Survey. arXiv preprint arXiv:2404.18930v2, 2025. https://arxiv.org/abs/2404.18930, Accessed: 2025\u201107\u201127.\u00a0\u21a9</p> </li> <li> <p>See footnote 17.\u00a0\u21a9</p> </li> <li> <p>The Decoder. People buy brand-new Chevrolets for $1 from a ChatGPT chatbot. The Decoder, 2025. https://the-decoder.com/people-buy-brand-new-chevrolets-for-1-from-a-chatgpt-chatbot/, Accessed: 2025\u201107\u201126.\u00a0\u21a9</p> </li> <li> <p>Harwell, Drew. X ordered its Grok chatbot to 'tell like it is.' Then the Nazi tirade began., July 11, 2025. https://www.washingtonpost.com/technology/2025/07/11/grok-ai-elon-musk-antisemitism/, Accessed: 2025\u201107\u201126.\u00a0\u21a9</p> </li> <li> <p>Threat Hunter Team. AI: Advent of Agents Opens New Possibilities for Attackers. Threat Intelligence Blog (Symantec / Broadcom), March 13 2025. https://www.security.com/threat-intelligence/ai-agent-attacks, Accessed: 2025\u201107\u201127.\u00a0\u21a9</p> </li> <li> <p>Goldman. A customer support AI went rogue\u2014and it's a warning for every company. Fortune, April 2025. https://fortune.com/article/customer-support-ai-cursor-went-rogue/, Accessed: 2025\u201107\u201127.\u00a0\u21a9</p> </li> <li> <p>See footnote 9.\u00a0\u21a9</p> </li> <li> <p>Kulp, Patrick. AI agents may be vulnerable to financial attacks. Tech Brew (Emerging Tech Brew), May 29 2025. https://www.emergingtechbrew.com/stories/2025/05/29/ai-agents-vulnerable-financial-attacks, Accessed: 2025\u201107\u201127.\u00a0\u21a9</p> </li> <li> <p>Alizadeh et al. Simple Prompt Injection Attacks Can Leak Personal Data Observed by LLM Agents During Task Execution. arXiv preprint arXiv:2506.01055, 2025. https://arxiv.org/pdf/2506.01055, Accessed: 2025\u201107\u201127.\u00a0\u21a9</p> </li> <li> <p>Chen et al. AI Agents Are Here\u2014So Are the Threats: Unit 42 Unveils the Top 10 Agentic\u2011AI Security Risks. Palo Alto Networks Unit 42 blog, 2025. https://unit42.paloaltonetworks.com/agentic-ai-threats/, Accessed: 2025\u201107\u201127.\u00a0\u21a9</p> </li> <li> <p>Delaney, Max. Google's AI Overviews are often so confidently wrong that I've lost all trust in them. TechRadar, 2025. https://www.techradar.com/computing/artificial-intelligence/googles-ai-overviews-are-often-so-confidently-wrong-that-ive-lost-all-trust-in-them, Accessed: 2025\u201107\u201127.\u00a0\u21a9</p> </li> <li> <p>Mudryi et al. The Hidden Dangers of Browsing AI Agents. arXiv preprint arXiv:2505.13076v1, 2025. https://arxiv.org/html/2505.13076v1, Accessed: 2025\u201107\u201127. Martin, Jason. Indirect Prompt Injection of Claude Computer Use. HiddenLayer blog, 2025. https://hiddenlayer.com/innovation-hub/indirect-prompt-injection-of-claude-computer-use/, Accessed: 2025\u201107\u201127.\u00a0\u21a9</p> </li> <li> <p>Yang et al. RiOSWorld: Benchmarking the Risk of Multimodal Computer\u2011Use Agents. arXiv preprint arXiv:2506.00618, 2025. https://arxiv.org/html/2506.00618v3, Accessed: 2025\u201107\u201127.\u00a0\u21a9</p> </li> <li> <p>Park, Sean. Unveiling AI Agent Vulnerabilities Part III: Data Exfiltration. TrendMicro, May 2025.  https://www.trendmicro.com/vinfo/sg/security/news/threat-landscape/unveiling-ai-agent-vulnerabilities-part-iii-data-exfiltration, Accessed: 2025-07-29.\u00a0\u21a9</p> </li> <li> <p>Unit 42. GitHub Actions Supply Chain Attack: A Targeted Attack on Coinbase Expanded to the Widespread tj-actions/changed-files Incident: Threat Assessment. Palo Alto Networks, Apr 2025.  https://unit42.paloaltonetworks.com/github-actions-supply-chain-attack/, Accessed: 2025-08-04.\u00a0\u21a9</p> </li> <li> <p>Munoz, Alvaro. GHSL-2024-294: Environment variable injection leading to potential secret exfiltration and privilege escalation in Azure/cli. Security Lab, Dec 2024. https://securitylab.github.com/advisories/GHSL-2024-294_Azure-cli/?utm_source=chatgpt.com, Accessed: 2025-08-04.\u00a0\u21a9</p> </li> <li> <p>He et al. Red-Teaming LLM Multi-Agent Systems via Communication Attacks. arXiv preprint arXiv:2502.14847, 2025. https://arxiv.org/pdf/2502.14847, Accessed: 2025\u201107\u201127.\u00a0\u21a9</p> </li> <li> <p>Kong et al. A Survey of LLM-Driven AI Agent Communication: Protocols, Security Risks, and Defense Countermeasures. arXiv preprint arXiv:2506.19676, 2025. https://arxiv.org/html/2506.19676, Accessed: 2025\u201107\u201127.\u00a0\u21a9</p> </li> <li> <p>Ferrag et al. From Prompt Injections to Protocol Exploits: Threats in LLM-Powered AI Agents Workflows. arXiv preprint arXiv:2506.23260, 2025. https://arxiv.org/html/2506.23260, Accessed: 2025\u201107\u201127.\u00a0\u21a9</p> </li> <li> <p>Spracklen et al. We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs. USENIX Security Symposium 2025 (preprint), 2025. https://www.usenix.org/system/files/conference/usenixsecurity25/sec25cycle1-prepub-742-spracklen.pdf, Accessed: 2025\u201107\u201127. METR. Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity. METR blog, July 10 2025. https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/, Accessed: 2025\u201107\u201127. Guo et al. RedCode: Risky Code Execution and Generation Benchmark for Code Agents. NeurIPS 2024 Datasets and Benchmarks Track, 2024. https://proceedings.neurips.cc/paper_files/paper/2024/hash/bfd082c452dffb450d5a5202b0419205-Abstract-Datasets_and_Benchmarks_Track.html, Accessed: 2025\u201107\u201127.\u00a0\u21a9</p> </li> <li> <p>Peng et al. CWEVAL: Outcome\u2011driven Evaluation on Functionality and Security of LLM Code Generation. arXiv preprint arXiv:2501.08200, 2025. https://arxiv.org/pdf/2501.08200, Accessed: 2025\u201107\u201127. Dilgren et al. SecRepoBench: Benchmarking LLMs for Secure Code Generation in Real-World Repositories. arXiv preprint arXiv:2504.21205, 2025. https://arxiv.org/html/2504.21205v1, Accessed: 2025\u201107\u201127.\u00a0\u21a9</p> </li> <li> <p>Pedro et al. Holodeck: Prompt-to-SQL Injections in LLM-Integrated Web Applications: Risks and Defenses. ICSE 2025 research track. https://syssec.dpss.inesc-id.pt/papers/pedro_icse25.pdf, Accessed: 2025\u201107\u201127.\u00a0\u21a9</p> </li> <li> <p>Ramirez et al. Which LLM Writes the Best SQL? Benchmarking analytical SQL generation by LLMs. Tinybird Blog, 2025. https://www.tinybird.co/blog-posts/which-llm-writes-the-best-sql, Accessed: 2025\u201107\u201127.\u00a0\u21a9</p> </li> <li> <p>Poireault, Kevin. Microsoft 365 Copilot hit by a zero\u2011click AI vulnerability allowing data exfiltration. Infosecurity Magazine, 2025. https://www.infosecurity-magazine.com/news/microsoft-365-copilot-zeroclick-ai/, Accessed: 2025\u201107\u201127.\u00a0\u21a9</p> </li> <li> <p>diskordia. Inside CVE-2025-32711 (EchoLeak): Prompt injection meets AI exfiltration. Hack The Box Blog, 2025. https://www.hackthebox.com/blog/cve-2025-32711-echoleak-copilot-vulnerability, Accessed: 2025\u201107\u201127. Burgess, Matt. Here Come the AI Worms. WIRED, 2025. https://www.wired.com/story/here-come-the-ai-worms/, Accessed: 2025\u201107\u201127.\u00a0\u21a9</p> </li> <li> <p>Kim et al. Prompt Flow Integrity to Prevent Privilege Escalation in LLM Agents. arXiv preprint arXiv:2503.15547v1, 2025. https://arxiv.org/html/2503.15547v1, Accessed: 2025\u201107\u201127.\u00a0\u21a9</p> </li> <li> <p>Kon et al. IaC-Eval: A Code Generation Benchmark for Cloud Infrastructure-as-Code Programs. NeurIPS 2024 poster, 2024. https://neurips.cc/virtual/2024/poster/97835, Accessed: 2025\u201107\u201127. Romeo et al. ARPaCCino: An Agentic-RAG for Policy as Code Compliance. arXiv preprint arXiv:2507.10584v1, 2025. https://arxiv.org/html/2507.10584v1, Accessed: 2025\u201107\u201127.\u00a0\u21a9</p> </li> <li> <p>Zhang et al. Breaking Agents: Compromising Autonomous LLM Agents Through Malfunction Amplification. arXiv preprint arXiv:2407.20859v1, 2024. https://arxiv.org/html/2407.20859v1, Accessed: 2025\u201107\u201127. OWASP. LLMRISK\u2011102025: Unbounded Consumption. OWASP GenAI Risk Database, 2025. https://genai.owasp.org/llmrisk/llm102025-unbounded-consumption/, Accessed: 2025\u201107\u201127.\u00a0\u21a9</p> </li> </ol>"},{"location":"implementation/","title":"Implementating the ARC Framework","text":"<p>A well-known adage in the Singapore civil service is \"Policy is implementation and implementation is policy\",<sup>1</sup> and this is resoundingly true for AI governance. The ARC Framework aims to bridge the gap between academic work on agentic AI safety and security with real-world governance considerations by synthesising a systematic way to:</p> <ul> <li>Anticipate safety and security risks before they manifest</li> <li>Scale governance across diverse AI applications</li> <li>Maintain consistency while allowing innovation</li> <li>Focus human oversight on critical decisions</li> <li>Adapt to technological change without rebuilding governance</li> </ul> <p>Click here for a downloadable version of the Agentic Risk &amp; Capability Framework - we provide the taxonomy, risk mapping, and technical controls in Google Sheets so that it's easy to download and adapt it.</p> <p>Here, we outline how two different parties, AI developers and governance teams, can apply the ARC Framework for their own use. </p> <ul> <li>For governance teams, the ARC Framework provides a scalable and sustainable way to manage the immediate and emergent risks of agentic AI systems in their organisation. </li> <li>For AI developers, the ARC Framework helps in identifying the relevant risks and controls to mitigate the safety and security risks for their agentic AI system. </li> </ul> <ol> <li> <p>Ho, Peter. Opening Address at 2010 Administrative Service Dinner and Promotion Ceremony. Public Service Division, March 2010. https://www.psd.gov.sg/files/opening-address-by-mr-peter-ho-at-2010-administrative-service-dinner-and-promotion-ceremony.pdf. Accessed: 2025-07-21.\u00a0\u21a9</p> </li> </ol>"},{"location":"implementation/for-ai-developers/","title":"For AI Developers","text":"<p>For teams building agentic AI systems, the ARC Framework provides a good starting point for thinking through the risks and controls required to manage safety and security risks. Instead of having to brainstorm a laundry list from scratch, teams can use the ARC Framework and adapt it to the specifics of their agentic AI system.</p> <p>The aim of this section is to help AI development teams understand and apply the ARC Framework to their own agentic AI systems. The first sub-section provides a general explanation of how teams can apply the ARC Framework to their own agentic AI systems, and the second sub-section provides an example to illustrate how this would work for real-world agentic AI systems. </p>"},{"location":"implementation/for-ai-developers/#applying-the-framework","title":"Applying the framework","text":"<p>In the subsequent parts, we provide a step-by-step approach for how AI developers should apply the ARC Framework to their team's agentic AI system. </p>"},{"location":"implementation/for-ai-developers/#step-1-identify-capabilities","title":"Step 1: Identify capabilities","text":"<p>Begin by analysing the agentic AI system's capabilities using either our capability taxonomy or the one provided by your organisation. </p> <p>The easiest way to do this is to list the functions that the agentic AI system performs and compare that to the list of capabilities in the taxonomy. For example, if the system sends emails autonomously to other users, that would clearly fall under \"Interaction - Official Commmunications\", while being able to generate and execute Python code would qualify for \"Operational - Code Execution\". </p> <p>Note that the capabilities are defined on a system-level, so even if only one agent has that capability, that means that the system as a whole has that capability. </p>"},{"location":"implementation/for-ai-developers/#step-2-evaluate-risks","title":"Step 2: Evaluate risks","text":"<p>Next, for each identified capability, map the specific risks that could arise from the system having that capability using the ARC Framework's risk mapping or the one provided by your organisation. The baseline risks should also be included here.</p> <p>Teams might find it helpful to further contextualise the risks to the use case. For example, defining what \"undesirable\" content means is important as such definitions are typically culturally specific, and focusing on the most critical topics when evaluating the risk of hallucination (e.g. medical advice, financial products) makes the risk more tractable and concrete. Additionally, there may be other safety or security risks that are not included in our list - to brainstorm for such risks, consider thinking through failure modes by asking yourself \"What is the worst scenario that could happen if this capability malfunctions or is misused?\"</p> <p>After compiling a comprehensive list of all the safety and security risks that could apply, teams should then apply the relevance criteria to identify and retain only the relevant risks for consideration. </p>"},{"location":"implementation/for-ai-developers/#step-3-adopt-or-adapt-controls","title":"Step 3: Adopt or adapt controls","text":"<p>Now for each relevant risk, review the recommended technical controls which help to mitigate the risk to an acceptable level - in the ARC Framework, we provide both baseline and capability controls. </p> <p>Similar to the risks in Step 2, these controls will also require some contextualisation. Not all controls are feasible or effective in every case, and teams should exercise judgement in deciding how to implement the control to meaningfully address the risk.<sup>1</sup> </p>"},{"location":"implementation/for-ai-developers/#step-4-assess-residual-risks","title":"Step 4: Assess residual risks","text":"<p>With the controls in place, teams should assess what the residual risks are and whether these are acceptable. As this is highly dependent on the use case, we do not provide a standard template for this assessment.<sup>2</sup> Some questions that teams may consider include identifying the scenarios where harm can still occur despite the controls and what the limitations of the controls are.</p> <p>If teams find that the residual risks are still unacceptable, then more controls would be needed to reduce either the likelihood or impact of the risk. </p>"},{"location":"implementation/for-ai-developers/#example-agentic-fact-checker","title":"Example: Agentic Fact Checker","text":"<p>Fact-checking is a very time-consuming process, requiring deep thought about each claim, what evidence there is for that claim, and whether it is sufficient to support that claim. Our colleagues in the AI Practice recently developed an agentic fact-checking system (read their Agentic AI Primer for details), using a complex multi-agent architecture with capabilities spanning cognitive reasoning, web search, database operations, and natural language generation to validate the veracity of any claim that is made. </p>"},{"location":"implementation/for-ai-developers/#step-1-identify-capabilities_1","title":"Step 1: Identify capabilities","text":"<p> Figure 1: Architecture for the Agentic Fact Checker (taken from the Agentic AI Primer)</p> <p>From Figure 1 above, we can see that the Agentic Fact Checker system comprises six distinct agents:</p> Agent Type Agent Name Task Core Orchestration Planner Coordinates overall fact-checking workflow and task distribution Core Orchestration Answer Decomposer Breaks down complex statements into verifiable claims Verification Agents Check-worthy Assessor Evaluates which claims require fact-checking Verification Agents Evidence Assessor Synthesizes information from multiple sources to make factuality determinations Information Retrieval Agents Web Searcher Accesses external web sources via search APIs Information Retrieval Agents Internal KB Searcher Queries internal knowledge base for relevant information <p>Based on the capability taxonomy from the ARC Framework, this system demonstrates the following capabilities:</p> Category Capability Explanation Cognitive Reasoning &amp; Problem-Solving The Answer Decomposer, Check-worthy Assessor, and Evidence Assessor agents conduct multi-step logical analyses of claims and evidence. Cognitive Planning &amp; Goal Management The Planner agent assesses the entire text and determines whether a systematic decomposition of claims is needed. Cognitive Tool Use &amp; Delegation The Evidence Assessor agent determines whether to searc the Internet or the internal knowledge base to validate the claim. Interaction Natural Language Communication All agents process text statements, while the final Evidence Assessor agent generates the factuality assessment. Interaction Internet &amp; Search Access The Web Searcher agent retrieves up-to-date information from web sources. Operational File &amp; Data Management The Internal KB Searcher queries and accesses internal knowledge bases."},{"location":"implementation/for-ai-developers/#step-2-evaluate-risks_1","title":"Step 2: Evaluate risks","text":"<p>Next, we identify the risks arising from both the baseline components and the capabilities, and assess how likely and impactful the risk is given the context of the Agentic Fact Checker. Here, we assume that the relevance threshold set by the organisation is (3*3), and the scores show the <code>impact_score * likelihood_score</code>.</p> Category Component / Capability Risk Assessment Baseline LLM Poorly aligned LLMs may pursue objectives which technically satisfy instructions but violate safety principles [Relevance: 2 * 1] Could lead to accepting biased sources as authoritative or prioritizing speed over accuracy in verification. However, fact-checking is relatively low-stakes compared to financial transactions or safety-critical systems. Baseline LLM Weaker LLMs have a higher tendency to produce unpredictable outputs which make agent behaviour erratic [Relevance: 2 * 1] Erratic behavior would primarily result in inconsistent fact-checking quality rather than serious harm. Users can typically verify claims independently if suspicious of results. Baseline LLM LLMs with poor safety tuning are more susceptible to prompt injection attacks and jailbreaking attempts [Relevance: 4 * 4] Particularly relevant given web search capability. Malicious websites could inject prompts to manipulate verification results, potentially spreading misinformation systematically. Baseline LLM Using LLMs trained on poisoned or biased data introduces manipulation risk, discriminatory decisions, or misinformation [Relevance: 3 * 2] Could result in systematic bias in fact-checking, but impact is limited to information accuracy rather than physical or financial harm. Users can always seek alternative verification. Baseline Tools Poorly implemented tools may not correctly verify user identity or permissions when executing privileged actions [Relevance: 2 * 2] Fact-checker has no privileged actions. Most operations are read-only searches. Limited potential for serious harm beyond information access. Baseline Tools Rogue tools that mimic legitimate ones can contain hidden malicious code that executes when loaded [Relevance: 2 * 2] While concerning, the fact-checker's tools are primarily search APIs with limited system access. Potential harm is constrained compared to systems with file modification or transaction capabilities. Baseline Tools Tools that do not properly sanitise or validate inputs can be exploited through prompt injection attacks [Relevance: 4 * 5] Critical given web search functionality. Unsanitized web content could compromise the entire verification process, leading to systematic misinformation propagation. Baseline Tools Tools that demand broader permissions than necessary create unnecessary attack surfaces for malicious actors [Relevance: 3 * 1] Fact-checking tools require minimal system permissions. Even if compromised, limited potential for serious system-wide damage. Baseline Instructions Simplistic instructions with narrow metrics and without broader constraints may result in agents engaging in specification gaming, resulting in poor performance or safety violations [Relevance: 2 * 1] Could lead to gaming verification metrics (e.g., always marking claims as \"uncertain\" to avoid errors) but unlikely to cause serious harm beyond reduced system utility. Baseline Instructions Vague instructions may compel agents to attempt to fill in missing constraints, resulting in unpredictable actions or incorrect steps taken [Relevance: 1 * 1] Scope of the system is clear and relatively simple, reducing the likelihood of unpredictable actions. Baseline Instructions Instructions without a clear distinction between system prompts and user requests may confuse agents and result in greater vulnerability to prompt injection attacks [Relevance: 3 * 2] Relevant given user-submitted claims for verification. Could allow users to manipulate the verification process, though impact is limited to information accuracy. Baseline Memory Malicious actors can inject false or misleading facts into the knowledge base, resulting in the agent acting on incorrect data or facts [Relevance: 3 * 2] Poisoned knowledge bases systematically bias future fact-checking decisions. However, cross-validation with web sources provides some protection against relying solely on compromised internal data. Baseline Memory Agents may inadvertently store sensitive user or organisational data from prior interactions, resulting in data privacy risks [Relevance: 2 * 1] Fact-checking typically involves public claims rather than sensitive personal data. Privacy risks are minimal unless checking claims about individuals. Baseline Memory Agents may mistakenly save momentary glitches and hallucinations into memory, resulting in compounding mistakes when the agent relies on the incorrect information for its decision or actions [Relevance: 1 * 1] Memory component in this system is not modified in response to or after the system's actions. Baseline Agentic Architecture In linear agentic pipelines where each stage blindly trusts the previous stage, single early mistakes may be propagated and magnified [Relevance: 3 * 2] Early errors in claim decomposition could propagate through verification steps, but the multi-agent architecture provides some redundancy and cross-checking opportunities. Baseline Agentic Architecture In hub-and-spoke architectures which route all decisions through one controller agent, any bug or compromise may distributes faulty instructions across the entire system [Relevance: 1 * 1] Agentic Fact Checker's architecture is more linear than hub-and-spoke. Baseline Agentic Architecture More complex agentic architectures may make it difficult to fully reconstruct decision processes across multiple agents [Relevance: 2 * 2] While transparency is important for fact-checking credibility, inability to trace decisions doesn't pose serious safety risks. Users can still evaluate final outputs independently. Cognitive Reasoning &amp; Problem-Solving Becoming ineffective, inefficient, or unsafe due to overthinking [Relevance: 3 * 1] Overthinking may be more likely for more complex claims or texts, potentially resulting in circular or faulty reasoning. Cognitive Reasoning &amp; Problem-Solving Engaging in deceptive behaviour through pursuing or prioritising other goals [Relevance: 2 * 1] Could lead to deliberately misleading fact-checks, but the multi-agent verification process provides some protection against single-agent deception. Cognitive Planning &amp; Goal Management Devising plans that are not effective in meeting the user's requirements [Relevance: 1 * 1] Ineffective verification plans would reduce system utility but users can recognize obviously inadequate verification attempts. Cognitive Planning &amp; Goal Management Devising plans that do not adhere to common sense or implicit assumptions about the user's instructions [Relevance: 1 * 1] May result in unnecessarily complex verification of simple claims or missing obvious evidence, but harm is limited to reduced efficiency and user frustration. Cognitive Tool Use &amp; Delegation Assigning tasks incorrectly to other agents [Relevance: 1 * 1] Task misassignment might reduce verification quality but is unlikely to cause serious harm given the read-only nature of most fact-checking operations. Cognitive Tool Use &amp; Delegation Attempting to use other agents maliciously [Relevance: 1 * 1] Limited potential for serious harm given the constrained scope of fact-checking operations. Most concerning would be attempts to bias verification results. Interaction Natural Language Communication Generating undesirable content (e.g. toxic, hateful, sexual) [Relevance: 1 * 2] Fact-checking outputs are typically factual assessments rather than creative content, and the tool is largely for internal use. Interaction Natural Language Communication Generating unqualified advice in specialised domains (e.g. medical, financial, legal) [Relevance: 2 * 3] Could provide medical or legal \"facts\" without appropriate disclaimers, potentially leading users to make harmful decisions based on unqualified information, but the tool is for internal use. Interaction Natural Language Communication Generating controversial content (e.g. political, competitors) [Relevance: 1 * 2] Fact-checking may involve controversial topics, but the tool is largely for internal use. Interaction Natural Language Communication Regurgitating personally identifiable information [Relevance: 1 * 2] Fact-checking typically focuses on public claims. PII exposure risk is minimal unless specifically checking claims about individuals. Interaction Natural Language Communication Generating non-factual or hallucinated content [Relevance: 4 * 4] Hallucinated \"facts\" in verification results directly undermine the system's core purpose and could spread misinformation if users trust the assessments. Interaction Natural Language Communication Generating copyrighted content [Relevance: 1 * 1] Fact-checking outputs are factual assessments rather than creative content. Minimal risk of copyright infringement in normal operation. Interaction Internet &amp; Search Access Opening vulnerabilities to prompt injection attacks via malicious websites [Relevance: 4 * 5] Critical vulnerability given the system's reliance on web sources. Malicious sites could inject prompts to manipulate verification results, potentially spreading coordinated misinformation. Interaction Internet &amp; Search Access Returning unreliable information or websites [Relevance: 5 * 4] Core risk for fact-checking systems. Accepting unreliable sources as authoritative directly undermines verification accuracy and could propagate misinformation. Operational File &amp; Data Management Overwriting or deleting database tables or files [Relevance: 1 * 1]: Fact-checking operations are all read-only, no write access to the database or files given. Operational File &amp; Data Management Overwhelming the database with poor, inefficient, or repeated queries [Relevance: 1 * 1] Would impact system performance but limited in scope to the system itself. Operational File &amp; Data Management Exposing personally identifiable or sensitive data from databases or files [Relevance: 1 * 1] Fact-checking databases typically contain public information. PII exposure risk is minimal unless the system stores sensitive verification requests. Operational File &amp; Data Management Opening vulnerabilities to prompt injection attacks via malicious data or files [Relevance: 4 * 4] If knowledge base contains user-contributed content, malicious data could inject prompts to bias future fact-checking decisions."},{"location":"implementation/for-ai-developers/#step-3-adopt-or-adapt-controls_1","title":"Step 3: Adopt or adapt controls","text":"<p>Given the relevance threshold of (3*3), we retain only the risks with a score higher than that. The table below shows the list of risks we have assessed to be very relevant and the corresponding controls.</p> Category Component / Capability Risk &amp; Assessment Controls Baseline LLM LLMs with poor safety tuning are more susceptible to prompt injection attacks and jailbreaking attempts  [Relevance: 4 * 4] Particularly relevant given web search capability. Malicious websites could inject prompts to manipulate verification results, potentially spreading misinformation systematically. \u2022 Implement input sanitisation measures or limit inputs to conventional ASCII characters only Baseline Tools Tools that do not properly sanitise or validate inputs can be exploited through prompt injection attacks  [Relevance: 4 * 5] Critical given web search functionality. Unsanitized web content could compromise the entire verification process, leading to systematic misinformation propagation. \u2022 Enforce strict schema validation (e.g. JSON schema, protobuf) and reject non\u2011conforming inputs upstream\u2022 Escape or encode user inputs when embedding into tool prompts or commands Interaction Natural Language Communication Generating non-factual or hallucinated content  [Relevance: 4 * 4] Hallucinated \"facts\" in verification results directly undermine the system's core purpose and could spread misinformation if users trust the assessments. \u2022 Implement methods to reduce hallucination rates (e.g. retrieval-augmented generation)\u2022 Implement UI/UX cues to highlight the risk of hallucination to the user\u2022 Implement features to enable users to easily verify the generated answer against the original content Interaction Internet &amp; Search Access Opening vulnerabilities to prompt injection attacks via malicious websites  [Relevance: 4 * 5] Critical vulnerability given the system's reliance on web sources. Malicious sites could inject prompts to manipulate verification results, potentially spreading coordinated misinformation. \u2022 Implement input guardrails to detect prompt injection or adversarial attacks\u2022 Implement escape filtering before including web content into prompts\u2022 Use structured retrieval APIs for searching the web rather than through web scraping Interaction Internet &amp; Search Access Returning unreliable information or websites  [Relevance: 5 * 4] Core risk for fact-checking systems. Accepting unreliable sources as authoritative directly undermines verification accuracy and could propagate misinformation. \u2022 Prioritise results from verified, high-quality domains (e.g. .gov, .edu, well-known publishers)\u2022 Require cross-source validation for some of the claims made Operational File &amp; Data Management Opening vulnerabilities to prompt injection attacks via malicious data or files  [Relevance: 4 * 4] If knowledge base contains user-contributed content, malicious data could inject prompts to bias future fact-checking decisions. \u2022 Validate new data used to supplement RAG databases or training data\u2022 Implement input guardrails to detect prompt injection or adversarial attacks\u2022 Disallow unknown or external files unless it is scanned"},{"location":"implementation/for-ai-developers/#step-4-assess-residual-risks_1","title":"Step 4. Assess residual risks","text":"<p>After evaluating the risks and the controls, we identified three key residual risks:</p> <ol> <li> <p>Sophisticated Prompt Injection remains a significant concern as advanced adversarial websites could craft content that bypasses current guardrails, potentially leading to complete compromise of the verification process for specific claims. While we have several technical controls in place, adversarial attack techniques continue to evolve and we cannot depend on these controls to fully mitigate the threat. To manage this residual risk, we will regularly monitor the system's API calls and outputs and flag anomalous activities.</p> </li> <li> <p>Cascading Hallucination presents a risk where early false information propagates through multiple verification steps, creating systematic bias in factuality assessments. This is particularly likely for novel or complex claims where existing knowledge bases may be incomplete, and cross-validation may not always catch sophisticated false reasoning that appears internally consistent. To manage this residual risk, we will implement a scoring and feedback mechanism in the front-end of the system to capture user feedback and to check against the system's internal reasoning for hallucinated outputs.</p> </li> <li> <p>Source Quality Assessment poses a high-likelihood risk, especially for technical or specialised subject areas where the system may struggle to distinguish between authoritative and unreliable sources. This could result in accepting false information as verified fact, undermining the system's core purpose. To manage this residual risk, we will start with small-scale pilots for the fact checker in specific domains first, validating its responses with subject matter experts, before expanding it for more general use.</p> </li> </ol> <p>These residual risks (and measures) will be periodically reviewed by the team to ensure relevance and effectiveness.</p> <ol> <li> <p>This is aligned to our recommendation to governance teams implementing the ARC Framework to take a comply-or-explain approach to the technical controls.\u00a0\u21a9</p> </li> <li> <p>Some helpful resources on managing residual risk include Verizon's article or Bitsight's article on residual risk.\u00a0\u21a9</p> </li> </ol>"},{"location":"implementation/for-governance-teams/","title":"For Governance Teams","text":"<p>In this section, we outline a step-by-step approach for applying the ARC Framework for your organisation and provide two worked examples to illustrate how it can be applied.</p>"},{"location":"implementation/for-governance-teams/#applying-the-arc-framework","title":"Applying the ARC Framework","text":""},{"location":"implementation/for-governance-teams/#step-1-review-the-capability-taxonomy","title":"Step 1: Review the capability taxonomy","text":"<p>Although we designed our capability taxonomy to be comprehensive in capturing the wide gamut of agentic capabilities today, we also recognise that there may be very niche capabilities which are especially relevant for specific domains. For example, companies operating in the intersection of agentic AI systems and robotics may have a whole umbrella of capabilities relating to hardware interaction, such as physical movement or object manipulation, while fintech companies may want more granularity for the capability to perform transactions by distinguishing between different types of transactions (e.g. cash, stocks, bonds).</p> <p>Governance teams should begin by reviewing the capability taxonomy and identifying the key capabilities which are intuitively most relevant to the organisation, and consider breaking out more specific second-level categories if the first-level categories are insufficient. We generally do not recommend organisations to drop any of the existing capabilities in the taxonomy, even if they are unlikely to be used, as this may hurt the taxonomy's generalisability across different verticals within the organisation (e.g. manufacturing, finance, or legal).</p>"},{"location":"implementation/for-governance-teams/#step-2-contextualise-the-risk-mapping","title":"Step 2: Contextualise the risk mapping","text":"<p>Each organisation has a different risk profile, and contextualising the risk mapping is important to ensure tight alignment between the organisation's risk management needs and the ARC Framework. For example, while the ARC Framework describes hallucination as a general risk for the capability of Natural Language Communication, it is more significant for a law firm rather than a social companion chatbot. </p> <p>Governance teams should go through the list of risks associated with each capability and apply the organisation's context (be it country- or industry-specific) to help the organisation and its internal AI development teams better understand the potential harms that could arise from such a capability. This is necessary if the capability taxonomy has been updated, since the current risk mapping is valid only for the present taxonomy.</p>"},{"location":"implementation/for-governance-teams/#step-3-adapt-the-controls","title":"Step 3: Adapt the controls","text":"<p>Similar to capabilities and risks, the technical controls themselves also require adaptation to the organisation's operating context and technical stack. For example, a fact-checking system operating in the European Union must comply with GDPR's strict data protection requirements, which might necessitate additional privacy controls beyond what's specified in the framework. Moreover, some controls may require specific technical expertise or infrastructure that is not available to all organisations, necessitating alternative approaches that achieve similar risk mitigation outcomes.</p> <p>Governance teams should systematically map their jurisdiction's AI and data protection requirements against the ARC Framework controls and conduct technical readiness assessments before mandating specific controls. For smaller teams, focusing initial efforts on controls that address the highest-impact risks within the organisation's regulatory and technical constraints may be more fruitful. Implementing audits which evaluate both compliance and efficacy will help to identify controls which may need to be adjusted and ensure that the framework stays relevant in the longer run.</p>"},{"location":"implementation/for-governance-teams/#step-4-define-relevance-criteria","title":"Step 4: Define relevance criteria","text":"<p>Not all risks are equally relevant, especially given the diversity of domains and use cases even within an organisation. For example, the risk of \"generating controversial content\" is minimal for an internal document processing system but represents a critical concern for a public-facing fact-checking platform that could influence public opinion. Relevance criteria provide a structured filter to help developers focus their limited time and resources on risks that actually matter for their specific system and use case.</p> <p>Under the ARC Framework, we recommend setting two main relevance criteria: impact (the magnitude of potential consequences) and likelihood (the probability of risk materialising). Each criterion is set along a five-point scale, and the final score is calculated by multiplying the two values together. This simplicity makes implementation easier. </p> <p>Governance teams should then calibrate these criteria to their organisation's context and risk appetite, and provide guidance on what score is required for a risk to be considered as \"relevant\".  For instance, a conservative financial institution might consider any risk scoring 6 or above (3x2 or 2x3) as relevant and requiring mandatory controls, while a technology startup might set the threshold at 9 or above (3x3). The calibration should also account for regulatory requirements - risks with direct compliance implications may be deemed relevant regardless of their calculated score. Additionally, governance teams should establish review periods to reassess these thresholds as the organisation's risk profile and business context evolve.</p>"},{"location":"implementation/for-governance-teams/#step-5-standardise-and-scale","title":"Step 5: Standardise and scale","text":"<p>Rolling out the ARC Framework - capability taxonomy, risk mapping, and technical controls - will not be an overnight feat, and requires significant change management and training across multiple departments. Governance teams should look to streamline the reporting process by providing a simple form or checklist for AI developers to declare their system's capabilities, relevant risks, and technical controls. As more data points for different systems start to stream in, governance teams will get a better organisation-wide view of system capabilities, risk exposures, and control adoption rates, which in turn will enhance the governance design process.</p> <p>Beyond the day-to-day operations of validating compliance to the framework, governance teams should leverage the framework to deliver more agile governance for agentic AI systems. For example, when new threats emerge or regulatory requirements change, governance teams can efficiently update the framework by adjusting impact levels, adding new risks, or enhancing control specifications, with changes automatically propagating to all systems through the next assessment cycle. This transforms AI governance from a reactive, project-by-project exercise into a proactive, portfolio-level capability that scales with organisational AI adoption and maintains effectiveness as both technology and threat landscapes evolve.</p>"},{"location":"implementation/for-governance-teams/#example-domain-specific-adaptations","title":"Example: Domain-specific adaptations","text":"<p>In this subsection, we provide two fictional examples of how an organisation can adapt the ARC Framework to their operating context. </p>"},{"location":"implementation/for-governance-teams/#healthcare-organisation","title":"Healthcare Organisation","text":"<p>Step 1: Review the capability taxonomy</p> <p>Healthcare organisations should expand the \"Natural Language Communication\" capability to distinguish between patient-facing communications and clinical decision support, as these may carry different regulatory implications (such as FDA medical device regulations and Singapore's Health Sciences Authority (\"HSA\") medical device guidelines). They may also need to add specific capabilities for medical image analysis under \"Multimodal Understanding &amp; Generation\" and create subcategories for different types of clinical data access under \"File &amp; Data Management\" (e.g., accessing electronic health records vs. medical literature databases). The \"Transactions\" capability should be refined to include medication ordering, appointment scheduling, and insurance claim processing, each with distinct risk profiles under healthcare regulations.</p> <p>Step 2: Contextualise the risk mapping</p> <p>Risk contextualisation must emphasize patient safety and data protection compliance. \"Generating unqualified advice in specialised domains\" becomes a critical risk requiring FDA medical device approval pathways or HSA therapeutic product registration rather than just disclaimer implementation. \"Regurgitating personally identifiable information\" takes on heightened significance due to HIPAA requirements in the US and Singapore's Personal Data Protection Act (PDPA) for healthcare data, with potential criminal penalties in both jurisdictions. \"Generating hallucinated content\" in clinical contexts could lead to patient harm, making this a patient safety issue under FDA guidelines. The organisation should map each risk against specific healthcare regulations (such as HIPAA, FDA 21 CFR Part 820, Joint Commission standards, HSA medical device regulations, MOH clinical practice guidelines) and patient safety frameworks.</p> <p>Step 3: Adapt the controls</p> <p>Controls must align with healthcare quality management systems and clinical workflows in both jurisdictions. \"Implement output guardrails\" becomes \"Implement clinical decision support alerts with physician override capabilities and audit trails compliant with prevailing regulations\", such as FDA 21 CFR Part 820 and HSA quality management system requirements. \"Require human approval\" transforms into \"Require licensed clinician review with documented clinical reasoning\". The organisation must integrate controls with existing clinical governance structures, electronic health record systems, and medical staff credentialing processes. Controls should also address clinical validation requirements, with mandatory testing against clinical datasets and integration with hospital incident reporting systems that in accordance with local requirements, be it the US Joint Commission sentinel event reporting or Singapore's MOH adverse event reporting requirements.</p> <p>Step 4: Define relevance criteria</p> <p>Healthcare organizations typically operate with very low risk tolerance due to patient safety implications and regulatory scrutiny in both the US and Singapore. The relevance threshold should be set conservatively - any risk scoring 4 or above (2x2) should be considered relevant given the potential for patient harm. Impact criteria must consider patient safety outcomes, regulatory compliance, and malpractice liability. Likelihood assessments should account for the clinical environment's high-stress conditions where users may over-rely on AI recommendations. All risks with direct patient safety implications should be deemed relevant regardless of calculated scores, especially for healthcare organisations in Singapore given Singapore's emphasis on healthcare quality through the National Healthcare Quality and Safety Framework.</p> <p>Step 5: Standardise and scale</p> <p>The rollout must integrate with existing clinical governance structures, including medical staff committees, quality assurance programs, and clinical informatics teams. Training should be incorporated into medical staff orientation and continuing education requirements. The framework should align with hospital accreditation standards and integrate with existing clinical audit processes. </p>"},{"location":"implementation/for-governance-teams/#manufacturing-company","title":"Manufacturing Company","text":"<p>Step 1: Review the capability taxonomy</p> <p>Manufacturing organisations should significantly expand operational capabilities to include industrial equipment control, sensor data processing, and supply chain coordination. Under \"System Management,\" they may need subcategories for industrial control systems, supervisory control and data acquisition systems, and manufacturing execution systems. The \"Transactions\" capability should be refined to distinguish between procurement, inventory management, and supplier communications, particularly important given Singapore's role as a regional manufacturing and logistics hub. They may also need to add capabilities for predictive maintenance, quality control analysis, and production optimization under a new \"Industrial Operations\" category.</p> <p>Step 2: Contextualise the risk mapping</p> <p>Risk contextualisation must emphasize operational safety, production continuity, and supply chain security. \"Misconfiguring system resources\" becomes critical as it could shut down production lines or damage expensive equipment, violating standards like OSHA's safety standards and Singapore's Workplace Safety and Health Act. \"Executing malicious code\" takes on heightened significance in industrial environments where cyberattacks could cause physical damage or safety incidents, subject to frameworks like NIST's cybersecurity frameworks or Singapore's Cybersecurity Act requirements for critical information infrastructure. Risks should be mapped against industrial safety standards (ISO 45001, OSHA regulations, Singapore's WSH Act), quality management systems (ISO 9001), and cybersecurity frameworks for industrial control systems (IEC 62443, NIST, Singapore's Cybersecurity Agency guidelines).</p> <p>Step 3: Adapt the controls</p> <p>Controls must integrate with existing industrial safety and quality management systems under both jurisdictions. \"Scope system privileges strictly\" becomes \"Implement role-based access control aligned with safety integrity levels and functional safety requirements\". \"Monitor system health metrics\" transforms into \"Integrate with plant-wide distributed control systems with automatic failsafe mechanisms compliant with prevailing regulations\". Controls should align with manufacturing execution systems, integrate with existing maintenance management systems, and comply with industrial cybersecurity standards. Safety-critical systems may require redundant control mechanisms and fail-safe designs to meet the requisite safety standards.</p> <p>Step 4: Define relevance criteria</p> <p>Manufacturing organisations must balance operational efficiency with safety and continuity requirements under both regulatory environments. Risk tolerance varies significantly between safety-critical systems (very low tolerance) and optimisation systems (moderate tolerance). The relevance threshold should be set at 6 or above (3x2 or 2x3) for most systems, but any risk affecting safety-critical operations should be relevant regardless of score. Impact criteria must consider production downtime costs, equipment damage potential, worker safety implications, and supply chain disruption effects. Likelihood assessments should account for the industrial environment's complexity and the potential for cascading physical failures.</p> <p>Step 5: Standardise and scale</p> <p>The rollout must integrate with existing plant engineering, maintenance, and safety management systems, and training should be incorporated into existing safety training programs and technical competency development. The framework should align with manufacturing quality management systems and integrate with existing operational risk management processes, with regular reviews coinciding with planned maintenance shutdowns and safety audit cycles, thereby ensuring that AI governance becomes part of standard operational excellence programs.</p>"},{"location":"intro/about_arc/","title":"About the ARC Framework","text":"<p>In this section, we provide a detailed overview of the ARC Framework.</p>"},{"location":"intro/about_arc/#objective","title":"Objective","text":"<p>The objective of the ARC Framework is to help organisations systematise how they identify and manage the risks of agentic AI systems. This is vital as companies begin to develop products and systems which provide LLMs with some agency to act autonomously on behalf of the organisation (which has a larger blast radius than with simpler LLM chatbots). </p>"},{"location":"intro/about_arc/#baseline-vs-capability-risks","title":"Baseline vs Capability Risks","text":"<p>One key aspect of the ARC Framework is in distinguishing between baseline and capability risks, where baseline risks apply to all agentic AI systems and capability risks apply to agentic AI systems with a given capability set<sup>1</sup>. In our framework, capability risks will vary depending on the actual capability set of the agentic AI system - systems with more (or riskier) capabilities will incur more capability risks.</p> <p>Note that each organisation has its own operating context - be it national (or state) regulations, industry-specific applications, or company-level considerations. As such, while we provide an initial list of both baseline and capability risks, these are meant only for reference and not as prescriptive guidance. Organisations should adapt these risks to their specific context, as we previously recommended for LLM safety testing.<sup>2</sup></p>"},{"location":"intro/about_arc/#baseline-risks","title":"Baseline Risks","text":"<p>All agents have common components, such as the LLM, tools, instructions, and memory,<sup>3</sup> and have similar design considerations, especially how agents are networked together and granted different roles within the system. Baseline risks capture the risks which arise from these two sources.</p> <p>For example, choosing a LLM which is less aligned to societal values results in greater risk of the agentic AI system misunderstanding instructions or performing unintended actions, while allowing a single \"super-user\" agent to access all private data incurs greater privacy risks. </p> <p>In the ARC Framework, we identify the baseline risks arising from the four components and two design considerations and provide two lists of baseline controls: one with fewer controls and another with more controls. Having two lists provides policy options for organisations - the shorter list may be enforced for all systems, while the longer list can be used either as best practices or mandated for more mission-critical systems. </p> <p>Find out more in our dedicated section on baseline risks.</p>"},{"location":"intro/about_arc/#capability-risks","title":"Capability Risks","text":"<p>Beyond the common components and design patterns, agentic AI systems often differ in the capabilities that they have to perform their intended tasks. For example, an agentic coding assistant would have capabilities in searching the Internet (for documentation), executing code in your computer, and generating answers to your questions, while a travel planning assistant may need to be able to book flights on your behalf or to email clarifying questions to hotels or tourist attractions. </p> <p>In the ARC Framework, we start with a taxonomy of capabilities which agentic AI systems may have. From each capability, we describe the specific risks that may arise due to that capability and suggest the corresponding technical controls that would help to mitigate those risks. The nexus between capabilities, risks, and controls is crucial because this provides adaptable guidance to system owners - agentic AI systems with more capabilities incur more risks and thus controls. </p> <p>Find out more in our dedicated section on capability risks.</p>"},{"location":"intro/about_arc/#implementation","title":"Implementation","text":"<p>Frameworks are only as useful when they are applied to real-world use. In designing this framework, we took much care to consider the practical constraints that organisations face as well as the competing concerns of governance teams and AI developers.</p> <p>To make the ARC Framework easier to implement, we provide detailed step-by-step approaches for how governance teams can implement the ARC Framework at an organisational level as well as how AI developers can apply the ARC Framework to manage the safety and security risks at a system level. We also go through detailed examples to illustrate how this would work in specific situations. </p> <p>Find out more in our dedicated section on implementation.</p> <ol> <li> <p>Goh et al. Measuring What Matters: A Framework for Evaluating Safety Risks in Real-World LLM Applications. https://arxiv.org/abs/2507.09820, 2025. Accessed: 2025-07-21.\u00a0\u21a9</p> </li> <li> <p>We deliberately borrow this phrase from Amartya Sen's Capability Approach, which this framework is partially inspired by. For more background, see https://iep.utm.edu/sen-cap/.\u00a0\u21a9</p> </li> <li> <p>This is adapted from OpenAI's guide to building agents (article), though we include the extra component of the memory or knowledge base.\u00a0\u21a9</p> </li> </ol>"},{"location":"intro/why_arc/","title":"Why the ARC Framework?","text":"<p>In this section, we explain why we developed the ARC framework, how it fits into the existing literature, and why we chose to focus on capabilities specifically.</p>"},{"location":"intro/why_arc/#background","title":"Background","text":"<p>OpenAI dubbed 2025 the \"year of the AI agent\u201d,<sup>1</sup> a prediction which quickly proved prescient. Within the first half of the year, major companies launched increasingly powerful systems that allow large language model (LLM) agents to reason, plan, and autonomously execute tasks such as code development or trip planning.<sup>2</sup> However, this surge in agent-driven AI innovation also brought renewed scrutiny to these systems' safety and security risks. Recent research has shown that LLM agents are often more prone to unsafe behaviors than their base models,<sup>3</sup> partly due to their growing autonomy and expanded capabilities through tool integration. As the ecosystem for agentic systems, including frameworks like Model Context Protocol<sup>4</sup>, continues to mature, there is greater urgency to establish robust governance mechanisms. </p>"},{"location":"intro/why_arc/#existing-work-on-technical-frameworks","title":"Existing Work on Technical Frameworks","text":"<p>As increased autonomy represents reduced determinism and predictability of agentic systems, many technical frameworks have focused on assessing the risks from a security perspective, adapting from cybersecurity literature. This includes Cloud Security Alliance's MAESTRO framework<sup>5</sup> and OWASP's whitepaper on agentic AI risks<sup>6</sup>, which focus on threat modeling approaches to identify risks and mitigations. These approaches detail security threats agentic systems could face (e.g., data poisoning, agent impersonation, cascading hallucination) and provide accompanying mitigations. Developers are expected to map out the system's architecture, and then identify attack vectors in each component from the list of risks. However, this is an onerous and challenging process for non-cybersecurity developers, as they are expected to enumerate across all possible risks for each component. NVIDIA's recommended taint tracing approach<sup>7</sup> similarly requires developers to identify security boundaries in the system's architecture with respect to the flow of untrusted data, assess the sensitivity of tools impacted by the flow and then adopt appropriate security controls accordingly. However, unlike OWASP and MAESTRO, the recommended controls are not as comprehensive and largely entail human oversight, which has severe limitations.</p> <p>Academic literature has focused on producing safety and security benchmarks to assess agentic AI risks. These benchmarks entail test scenarios or tasks that are expected to reveal specific risk behaviours of the agentic system. For example, CVEBench,<sup>8</sup> RedCode,<sup>9</sup> AgentHarm,<sup>10</sup>, AgentDojo<sup>11</sup> assess whether LLMs can complete cybersecurity attacks or harmful tasks like fraud. In these settings, agentic systems are assessed on whether they comply with and successfully complete multi-step requests. However, these benchmarks do not help developers systematically identify the risks and attack scenarios of their specific applications. Tool-based benchmarks that entail evaluating the risks of individual tools that agents have access to face similar problems. APIBench,<sup>12</sup> ToolSword,<sup>13</sup>, and ToolEmu<sup>14</sup>, evaluate the performance and safety of LLMs in utilizing and interacting with tools like <code>bash</code>. However, there exists agentic system risks unrelated to tool use (e.g., using a misaligned planning agent resulting in bad plans) which such benchmarks will not cover. As such, they are insufficient in helping developers assess agentic risks. </p> <p>In terms of technical controls for managing risks, AI control has emerged as a useful paradigm in preventing misaligned AI systems from causing harm.<sup>15</sup> Rather than relying solely on training techniques to shape model behavior, AI control focuses on designing mechanisms like monitoring and human approval to constrain AI systems. In line with this paradigm, some works have emerged to generate custom controls for specific applications. Progent introduces a language for flexibly expressing privilege control policies applied during execution for each agent.<sup>16</sup> The UK AI Security Institute similarly advocates for application-specific controls, which is derived from evaluating each system's threat model-specific capabilities based on models and deployment contexts.<sup>17</sup></p> <p>Instead of providing a list of risks and controls upfront, some works have instead published guides on best practices in evaluating and managing risks of agentic AI systems. OpenAI<sup>18</sup> shared best practices like constraining the agent's action spaces, setting default behaviors and ensuring attributabilty, as well as open problems for further investigation. Google<sup>19</sup> emphasises a hybrid defence-in-depth strategy that combines traditional, deterministic security measures with dynamic, reasoning-based defenses, and higlights core principles of human controls, agent permissions and observability in designing agentic systems. Similarly, Beurer-Kellner et al.<sup>20</sup> propose six design patterns for building AI agents with provable resistance to prompt injection, based on the principle that once an LLM agent has ingested untrusted input, it must be constrained so that it is impossible for that input to trigger any consequential actions.</p>"},{"location":"intro/why_arc/#what-is-different-about-the-arc","title":"What is different about the ARC?","text":"<p>The ARC builds on existing works to enable organisations to govern agentic systems effectively and provide practical guidance in assessing and mitigating the associated risks. Importantly, it attempts to structure the risks and controls identified in existing works into a unified and actionable framework. This is achieved mainly by including and differentiating between baseline and capability risks and controls. With a structured approach, the ARC attempts to strike an appropriate balance between:</p> <ul> <li>Thoroughly mapping out the risk landscape, and not being too inflexible to changes - Instead of listing out all possible agentic risks upfront, the ARC ties capabilities to risks, providing flexibility for identifying more risks as agentic systems acquire more capabilities</li> <li>Being applicable and adaptable to all agentic AI systems, and not being too generic or watered down - The ARC combines both generic (i.e., baseline) and specific (i.e., capability) risks and controls to ensure broad applicability and minimal standards while mantaining customisability</li> <li>Providing meaningful and concrete guidance on the risks and controls, and not being too prescriptive - Contextualising risk identification from a capabilities perspective is simpler and more straightforward for developers, resulting in practical and proportional risk assessment and mitigation</li> <li>Scalability at an organisation-level, and not simply being a paper exercise - The ARC provides an initial set of risks and controls to reduce compliance effort and ensure scalability across large organisations, while the customisation process ensures proper risk assessment</li> </ul> <ol> <li> <p>Hamilton, E. 2025 is the year of ai agents, OpenAI CPO says. Axios, January 2025. URL https://www.axios.com/2025/01/23/davos-2025-ai-agents. Accessed: 2025-05-11.\u00a0\u21a9</p> </li> <li> <p>Both Anthropic and Google released agentic tools (Claude Code and Gemini CLI respectively), built on top of their popular models. Most recently, OpenAI released ChatGPT Agent in July 2025 (article).\u00a0\u21a9</p> </li> <li> <p>See the following articles: (i) Chiang et al. Harmful helper: Perform malicious tasks? web AI agents might help. In ICLR 2025 Workshop on Building Trust in Language Models and Applications, 2025. URL https://openreview.net/forum?id=4KoMbO2RJ9, (ii) Kumar et al. Aligned LLMs are not aligned browser agents. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=NsFZZU9gvk, (iii) Yu, C. and Papakyriakopoulos, O. Safety devolution in AI agents. In ICLR 2025 Workshop on Human-AI Coevolution, 2025. URL https://openreview.net/forum?id=7nJmuFFkWd.\u00a0\u21a9</p> </li> <li> <p>Anthropic. Model Context Protocol (MCP). https://docs.anthropic.com/en/docs/agents-and-tools/mcp, 2024. Accessed: 2025-05-11.\u00a0\u21a9</p> </li> <li> <p>Ken\u202fHuang. Agentic AI Threat Modeling Framework: MAESTRO. https://cloudsecurityalliance.org/blog/2025/02/06/agentic-ai-threat-modeling-framework-maestro, 2025. Accessed: 2025\u201107\u201126.\u00a0\u21a9</p> </li> <li> <p>OWASP Foundation. Agentic AI \u2013 Threats and Mitigations. https://genai.owasp.org/resource/agentic-ai-threats-and-mitigations/, 2025. Accessed: 2025\u201107\u201126.\u00a0\u21a9</p> </li> <li> <p>Harang et al. Agentic Autonomy Levels and Security. https://developer.nvidia.com/blog/agentic-autonomy-levels-and-security/, 2025. Accessed: 2025\u201107\u201126.\u00a0\u21a9</p> </li> <li> <p>Zhang et al. Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risks of Language Models. https://arxiv.org/abs/2408.08926, 2024. Accessed: 2025-05-11.\u00a0\u21a9</p> </li> <li> <p>Guo et al. RedCode: Risky Code Execution and Generation Benchmark for Code Agents.\u202fNeurIPS\u202f2024 Datasets and Benchmarks Track. https://proceedings.neurips.cc/paper_files/paper/2024/hash/bfd082c452dffb450d5a5202b0419205\u2011Abstract\u2011Datasets_and_Benchmarks_Track.html, 2024. Accessed:\u202f2025\u201107\u201126.\u00a0\u21a9</p> </li> <li> <p>Andriushchenko et al. AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents. https://arxiv.org/abs/2410.09024, 2025. Accessed: 2025-05-11.\u00a0\u21a9</p> </li> <li> <p>Debenedetti et al. AgentDojo: A Dynamic Environment to Evaluate Prompt Injection Attacks and Defenses for LLM Agents. https://arxiv.org/abs/2406.13352, 2024. Accessed: 2025-05-11.\u00a0\u21a9</p> </li> <li> <p>Patil et al. Gorilla: Large Language Model Connected with Massive APIs. https://arxiv.org/abs/2305.15334, 2023. Accessed: 2025-05-11.\u00a0\u21a9</p> </li> <li> <p>Ye et al. ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages. https://arxiv.org/abs/2402.10753, 2024. Accessed: 2025-05-11.\u00a0\u21a9</p> </li> <li> <p>Ruan et al. Identifying the Risks of LM Agents with an LM-Emulated Sandbox. https://arxiv.org/abs/2309.15817, 2023. Accessed: 2025-05-11.\u00a0\u21a9</p> </li> <li> <p>Greenblatt et al. arXiv preprint arXiv:2312.06942, 2023. https://arxiv.org/abs/2312.06942, accessed: 2025\u201107\u201126.\u00a0\u21a9</p> </li> <li> <p>Shi et al. Progent: Programmable Privilege Control for LLM Agents. https://arxiv.org/abs/2504.11703, 2025. Accessed: 2025-05-11.\u00a0\u21a9</p> </li> <li> <p>Korbak et al. How to evaluate control measures for LLM agents? A trajectory from today to superintelligence. https://arxiv.org/abs/2504.05259, 2025. Accessed: 2025-05-11.\u00a0\u21a9</p> </li> <li> <p>Shavit et al. Practices for Governing Agentic AI Systems. OpenAI white paper, 2023. https://cdn.openai.com/papers/practices-for-governing-agentic-ai-systems.pdf, Accessed:\u202f2025\u201107\u201126.\u00a0\u21a9</p> </li> <li> <p>D\u00edaz et al. An Introduction to Google\u2019s Approach for Secure AI Agents. Google white paper, 2025. https://research.google/pubs/an-introduction-to-googles-approach-for-secure-ai-agents/, Accessed:\u202f2025\u201107\u201126.\u00a0\u21a9</p> </li> <li> <p>Beurer\u2011Kellner et al. Design\u202fPatterns\u202ffor\u202fSecuring\u202fLLM\u202fAgents\u202fagainst\u202fPrompt\u202fInjections. arXiv preprint arXiv:2506.08837, first posted June\u202f10,\u202f2025; revised versions through June\u202f27,\u202f2025. https://arxiv.org/abs/2506.08837, Accessed: 2025\u201107\u201126.\u00a0\u21a9</p> </li> </ol>"}]}